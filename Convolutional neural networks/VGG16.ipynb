{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3619,
     "status": "ok",
     "timestamp": 1729269361711,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "JKAgOVUI-NAt",
    "outputId": "9a3fe7df-32a9-448b-eb55-37f887b5b160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/phamdinhtrunghieu/.cache/kagglehub/datasets/marquis03/cats-and-dogs/versions/2/train\n",
      "['cat', '.DS_Store', 'classname.txt', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"marquis03/cats-and-dogs\") + \"/train\"\n",
    "# path = \"/Users/phamdinhtrunghieu/.cache/kagglehub/datasets/marquis03/cats-and-dogs/versions/2/train\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 6666,
     "status": "ok",
     "timestamp": 1729269368373,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "9OYjRCScIS4G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "  resize_transform = transforms.Resize((224, 224))\n",
    "\n",
    "  resized_img = resize_transform(img)\n",
    "\n",
    "  plt.figure(figsize=(8, 4))\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.imshow(img)\n",
    "  plt.title('Original Image')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.imshow(resized_img)\n",
    "  plt.title('Resized Image (224x224)')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10095,
     "status": "ok",
     "timestamp": 1729269378460,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "Y1uTwZGbInQt",
    "outputId": "a3e6b19a-509d-4eb0-a7c4-60d7873ea241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', '.DS_Store', 'classname.txt', 'dog']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = f\"{path}\"\n",
    "\n",
    "print(os.listdir(dataset_path))\n",
    "\n",
    "dataset = []\n",
    "\n",
    "count_label = 0\n",
    "\n",
    "number_of_label = 2\n",
    "\n",
    "labels = [\"cat\", \"dog\"]\n",
    "\n",
    "for label in labels:\n",
    "  data_path = f\"{dataset_path}/{label}\"\n",
    "  for example in os.listdir(data_path):\n",
    "    img = Image.open(f\"{data_path}/{example}\")\n",
    "    # show_image(img)\n",
    "    transform = transforms.Compose([\n",
    "                                    transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor()\n",
    "                                    ])\n",
    "    x = 1.0 * transform(img)\n",
    "    if (x.size()[0] != 3):\n",
    "      continue\n",
    "    y = torch.zeros(number_of_label)\n",
    "    y[count_label] = 1\n",
    "\n",
    "    # print(x,y)\n",
    "\n",
    "    dataset.append((x,y))\n",
    "\n",
    "  count_label += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1729269378462,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "DVTShB3AI6jd"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1729269379173,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "lEkjtZEqJnx9",
    "outputId": "de04de73-e0df-4bec-e3a9-03a61bb5727d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "max_w, max_h = 0, 0\n",
    "for x, y in dataset:\n",
    "  print(x.size())\n",
    "  print(y.size())\n",
    "  max_w = max(max_w, x.size()[1])\n",
    "  max_h = max(max_h, x.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1729269379173,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "rI_DMvbqJrLV",
    "outputId": "c6446750-fad6-46ef-d432-ef858248ac12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 224\n"
     ]
    }
   ],
   "source": [
    "print(max_h, max_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729269379173,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "aKob6pd1KhjZ"
   },
   "outputs": [],
   "source": [
    "class ClassifierVGG16(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv11 = nn.Conv2d(3, 64, 3, 1, 1) # 224\n",
    "    self.conv12 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "    self.maxpool1 = nn.MaxPool2d(2,2) # 112\n",
    "    \n",
    "    self.conv21 = nn.Conv2d(64, 128, 3, 1, 1) # 112\n",
    "    self.conv22 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "    self.maxpool2 = nn.MaxPool2d(2,2) # 56 \n",
    "    \n",
    "    self.conv31 = nn.Conv2d(128, 256, 3, 1, 1) # 56\n",
    "    self.conv32 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "    self.conv33 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "    self.maxpool3 = nn.MaxPool2d(2,2) # 28 \n",
    "    \n",
    "    self.conv41 = nn.Conv2d(256, 512, 3, 1, 1) # 28\n",
    "    self.conv42 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "    self.conv43 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "    self.maxpool4 = nn.MaxPool2d(2,2) # 14 \n",
    "    \n",
    "    self.conv51 = nn.Conv2d(512, 512, 3, 1, 1) # 14\n",
    "    self.conv52 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "    self.conv53 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "    self.maxpool5 = nn.MaxPool2d(2,2) # 7 \n",
    "    \n",
    "    self.fc1 = nn.Linear(7 * 7 * 512, 4096)\n",
    "    self.fc2 = nn.Linear(4096, 4096)\n",
    "    self.fc2 = nn.Linear(4096, 2)\n",
    "    \n",
    "\n",
    "  def forward(self,x):\n",
    "    x = f.relu(self.conv11(x))\n",
    "    x = f.relu(self.conv12(x))\n",
    "    x = self.maxpool1(x)\n",
    "    # print(x.size())\n",
    "    \n",
    "    x = f.relu(self.conv21(x))\n",
    "    x = f.relu(self.conv22(x))\n",
    "    x = self.maxpool2(x)\n",
    "    # print(x.size())\n",
    "    \n",
    "    x = f.relu(self.conv31(x))\n",
    "    x = f.relu(self.conv32(x))\n",
    "    x = f.relu(self.conv33(x))\n",
    "    x = self.maxpool3(x)\n",
    "    # print(x.size())\n",
    "    \n",
    "    x = f.relu(self.conv41(x))\n",
    "    x = f.relu(self.conv42(x))\n",
    "    x = f.relu(self.conv43(x))\n",
    "    x = self.maxpool4(x)\n",
    "    # print(x.size())\n",
    "    \n",
    "    x = f.relu(self.conv51(x))\n",
    "    x = f.relu(self.conv52(x))\n",
    "    x = f.relu(self.conv53(x))\n",
    "    x = self.maxpool5(x)\n",
    "    # print(x.size())\n",
    "\n",
    "    x = torch.flatten(x)\n",
    "    \n",
    "    x = f.relu(self.fc1(x))\n",
    "    x = f.relu(self.fc2(x))\n",
    "\n",
    "    return f.softmax(x, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729269379173,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "StYAEZFgcOiR"
   },
   "outputs": [],
   "source": [
    "def get_label(y):\n",
    "  res = 0\n",
    "\n",
    "  for id in range(number_of_label):\n",
    "    if y[res].item() < y[id].item():\n",
    "      res = id\n",
    "\n",
    "\n",
    "\n",
    "  return labels[res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 144303,
     "status": "error",
     "timestamp": 1729269523814,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "wcGpl0P6SoTQ",
    "outputId": "0938091b-ae60-4af5-8e6a-5db62d0d3d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "=====epoch:0=====\n",
      "Epoch 0 - 0: cat - cat || Loss: 0.812772274017334\n",
      "tensor([1., 0.]) tensor([0.5005, 0.4995], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 1: dog - cat || Loss: 0.813843309879303\n",
      "tensor([0., 1.]) tensor([0.5006, 0.4994], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 2: cat - cat || Loss: 0.8126890659332275\n",
      "tensor([1., 0.]) tensor([0.5006, 0.4994], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 3: cat - cat || Loss: 0.8126052021980286\n",
      "tensor([1., 0.]) tensor([0.5007, 0.4993], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 4: dog - cat || Loss: 0.8140859603881836\n",
      "tensor([0., 1.]) tensor([0.5008, 0.4992], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 5: dog - cat || Loss: 0.8141448497772217\n",
      "tensor([0., 1.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 6: cat - cat || Loss: 0.8124179840087891\n",
      "tensor([1., 0.]) tensor([0.5008, 0.4992], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 7: dog - cat || Loss: 0.8141622543334961\n",
      "tensor([0., 1.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 8: cat - cat || Loss: 0.812402069568634\n",
      "tensor([1., 0.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 9: dog - cat || Loss: 0.8141764402389526\n",
      "tensor([0., 1.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 10: cat - cat || Loss: 0.8123895525932312\n",
      "tensor([1., 0.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 11: dog - cat || Loss: 0.8141879439353943\n",
      "tensor([0., 1.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 12: cat - cat || Loss: 0.8123790621757507\n",
      "tensor([1., 0.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 13: cat - cat || Loss: 0.8123261332511902\n",
      "tensor([1., 0.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 14: cat - cat || Loss: 0.8121863007545471\n",
      "tensor([1., 0.]) tensor([0.5011, 0.4989], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 15: dog - cat || Loss: 0.8145551085472107\n",
      "tensor([0., 1.]) tensor([0.5013, 0.4987], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 16: dog - cat || Loss: 0.8146589994430542\n",
      "tensor([0., 1.]) tensor([0.5014, 0.4986], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 17: dog - cat || Loss: 0.8146605491638184\n",
      "tensor([0., 1.]) tensor([0.5014, 0.4986], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 18: dog - cat || Loss: 0.8145695328712463\n",
      "tensor([0., 1.]) tensor([0.5013, 0.4987], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 19: dog - cat || Loss: 0.8143954277038574\n",
      "tensor([0., 1.]) tensor([0.5011, 0.4989], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 20: cat - cat || Loss: 0.812376856803894\n",
      "tensor([1., 0.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 21: cat - cat || Loss: 0.8125087022781372\n",
      "tensor([1., 0.]) tensor([0.5008, 0.4992], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 22: dog - cat || Loss: 0.8139882683753967\n",
      "tensor([0., 1.]) tensor([0.5007, 0.4993], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 23: dog - cat || Loss: 0.813872218132019\n",
      "tensor([0., 1.]) tensor([0.5006, 0.4994], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 24: dog - cat || Loss: 0.8136756420135498\n",
      "tensor([0., 1.]) tensor([0.5004, 0.4996], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 25: cat - cat || Loss: 0.8131165504455566\n",
      "tensor([1., 0.]) tensor([0.5001, 0.4999], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 26: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 27: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 28: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 29: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 30: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 31: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 32: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 33: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 34: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 35: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 36: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 37: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 38: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 39: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 40: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 41: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 42: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 43: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 44: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 45: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 46: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 47: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 48: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 49: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 50: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 51: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 52: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 53: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 54: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 55: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 56: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 57: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 58: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 59: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 60: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 61: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 62: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 63: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 64: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 65: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 66: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 67: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 68: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 69: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 70: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 71: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 72: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 73: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 74: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 75: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 76: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 77: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 78: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 79: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 80: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 81: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 82: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 83: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 84: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 85: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 86: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 87: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 88: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 89: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 90: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 91: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 92: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 93: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 94: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 95: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 96: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 97: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 98: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 99: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 100: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 101: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 102: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 103: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 104: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 105: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 106: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 107: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 108: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 109: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 110: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 111: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 112: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 113: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 114: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 115: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 116: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 117: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 118: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 119: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 120: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 121: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 122: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 123: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 124: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 125: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 126: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 127: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 128: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 129: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 130: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 131: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 132: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 133: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 134: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 135: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 136: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 137: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 138: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 139: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 140: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 141: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 142: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 143: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 144: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 145: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 146: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 147: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 148: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 149: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 150: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 151: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 152: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 153: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 154: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 155: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 156: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 157: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 158: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 159: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 160: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 161: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 162: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 163: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 164: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 165: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 166: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 167: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 168: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 169: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 170: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 171: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 172: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 173: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 174: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 175: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 176: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 177: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 178: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 179: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 180: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 181: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 182: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 183: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 184: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 185: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 186: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 187: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 188: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 189: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 190: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 191: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 192: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 193: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 194: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 195: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 196: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 197: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 198: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 199: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 200: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 201: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 202: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 203: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 204: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 205: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 206: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 207: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 208: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 209: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 210: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 211: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 212: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 213: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 214: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 215: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 216: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 217: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 218: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 219: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 220: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 221: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 222: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 223: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 224: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 225: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 226: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 227: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 228: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 229: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 230: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 231: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 232: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 233: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 234: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 235: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 236: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 237: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 238: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 239: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 240: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 241: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 242: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 243: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 244: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 245: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 246: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 247: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 248: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 249: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 250: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 251: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 252: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 253: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 254: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 255: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 256: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 257: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 258: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 259: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 260: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 261: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 262: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 263: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 264: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 265: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 266: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 267: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 268: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 269: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 270: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 271: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 272: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 273: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 274: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 275: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 276: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 277: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 278: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 279: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 280: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 281: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 282: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 283: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 284: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 285: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 286: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 287: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 288: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 289: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 290: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 291: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 292: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 293: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 294: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 295: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 296: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 297: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 298: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 299: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 300: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 301: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 302: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 303: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 304: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 305: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 306: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 307: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 308: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 309: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 310: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 311: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 312: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 313: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 314: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 315: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 316: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 317: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 318: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 319: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 320: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 321: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 322: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 323: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 324: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 325: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 326: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 327: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 328: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 329: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 330: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 331: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 332: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 333: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 334: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 335: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 336: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 337: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 338: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 339: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 340: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 341: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 342: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 343: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 344: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 345: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 346: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 347: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 348: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 349: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 350: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 351: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 352: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 353: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 354: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 355: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 356: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 357: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 358: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 359: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 360: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 361: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 362: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 363: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 364: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 365: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 366: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 367: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 368: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 369: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "=====epoch:1=====\n",
      "Epoch 1 - 0: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 1: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 2: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 3: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 4: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 5: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 6: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 7: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 8: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 9: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 10: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 11: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 12: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 13: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 14: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 15: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 16: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 17: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 18: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 19: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 20: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 21: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 22: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 23: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 24: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 25: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 26: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 27: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 28: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 29: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 30: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 31: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 32: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 33: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 34: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 35: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 36: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 37: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 38: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 39: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 40: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 41: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 42: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 43: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 44: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 45: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 46: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 47: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 48: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 49: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 50: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 51: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 52: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 53: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 54: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 55: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 56: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 57: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 58: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 59: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 60: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 61: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 62: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 63: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 64: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 65: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 66: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 67: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 68: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 69: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 70: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 71: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 72: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 73: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 74: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 75: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 76: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 77: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 78: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 79: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 80: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 81: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 82: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 83: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 84: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 85: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 86: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 87: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 88: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 89: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 90: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 91: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 92: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 93: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 94: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 95: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 96: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 97: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 98: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 99: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 100: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 101: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 102: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 103: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 104: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 105: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 106: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 107: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 108: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 109: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 110: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 111: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 112: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 113: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 114: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 115: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 116: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 117: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 118: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 119: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 120: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 121: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 122: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 123: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 124: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 125: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 126: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 127: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 128: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 129: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 130: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 131: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 132: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 133: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 134: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 135: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 136: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 137: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 138: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 139: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 140: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 141: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 142: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 143: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 144: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 145: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 146: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 147: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 148: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 149: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 150: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 151: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 152: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 153: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 154: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 155: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 156: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 157: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 158: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 159: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 160: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 161: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 162: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 163: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 164: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 165: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 166: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 167: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 168: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 169: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 170: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 171: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 172: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 173: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 174: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 175: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 176: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 177: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 178: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 179: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 180: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 181: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 182: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 183: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 184: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 185: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 186: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 187: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 188: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 189: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 190: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 191: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 192: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 193: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 194: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 195: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 196: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 197: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 198: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 199: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 200: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 201: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 202: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 203: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 204: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 205: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 206: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 207: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 208: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 209: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 210: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 211: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 212: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 213: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 214: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 215: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 216: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 217: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 218: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 219: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 220: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 221: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 222: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 223: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 224: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 225: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 226: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 227: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 228: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 229: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 230: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 231: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 232: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 233: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 234: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 235: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 236: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 237: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 238: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 239: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 240: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 241: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 242: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 243: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 244: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 245: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 246: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 247: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 248: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 249: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 250: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 251: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 252: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 253: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 254: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 255: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 256: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 257: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 258: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 259: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 260: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 261: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 262: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 263: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 264: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 265: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 266: dog - cat || Loss: 0.8132616281509399\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 267: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 268: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 269: cat - cat || Loss: 0.8132616281509399\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_label(y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_label(y_hat)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m || Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(y, y_hat)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/sgd.py:123\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m momentum_buffer_list: List[Optional[Tensor]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    119\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    120\u001b[0m     group, params, grads, momentum_buffer_list\n\u001b[1;32m    121\u001b[0m )\n\u001b[0;32m--> 123\u001b[0m sgd(\n\u001b[1;32m    124\u001b[0m     params,\n\u001b[1;32m    125\u001b[0m     grads,\n\u001b[1;32m    126\u001b[0m     momentum_buffer_list,\n\u001b[1;32m    127\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    128\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    129\u001b[0m     lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    130\u001b[0m     dampening\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdampening\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    131\u001b[0m     nesterov\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnesterov\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    132\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    133\u001b[0m     has_sparse_grad\u001b[38;5;241m=\u001b[39mhas_sparse_grad,\n\u001b[1;32m    134\u001b[0m     foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    135\u001b[0m     fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    136\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    137\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, momentum_buffer_list):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/sgd.py:299\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, fused, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 299\u001b[0m func(\n\u001b[1;32m    300\u001b[0m     params,\n\u001b[1;32m    301\u001b[0m     d_p_list,\n\u001b[1;32m    302\u001b[0m     momentum_buffer_list,\n\u001b[1;32m    303\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    304\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[1;32m    305\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    306\u001b[0m     dampening\u001b[38;5;241m=\u001b[39mdampening,\n\u001b[1;32m    307\u001b[0m     nesterov\u001b[38;5;241m=\u001b[39mnesterov,\n\u001b[1;32m    308\u001b[0m     has_sparse_grad\u001b[38;5;241m=\u001b[39mhas_sparse_grad,\n\u001b[1;32m    309\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    310\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    311\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/sgd.py:352\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m         grad \u001b[38;5;241m=\u001b[39m buf\n\u001b[0;32m--> 352\u001b[0m param\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ClassifierVGG16()\n",
    "# model = VGG16()\n",
    "print(get_label(model(x)))\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "loss_calculator = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "  number_of_example = len(dataset)\n",
    "  number_of_correct = 0\n",
    "  print(f\"=====epoch:{epoch}=====\")\n",
    "  for id in range(number_of_example):\n",
    "    x, y = dataset[id]\n",
    "    y_hat = model(x)\n",
    "\n",
    "    # loss = ((y - y_hat) ** 2).mean()\n",
    "    loss = loss_calculator(y, y_hat)\n",
    "\n",
    "    if get_label(y_hat) == get_label(y):\n",
    "      number_of_correct += 1\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch} - {id}: {get_label(y)} - {get_label(y_hat)} || Loss: {loss.item()}\")\n",
    "    print(y, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeh8+7BqFjyDtE333ycs6T",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
