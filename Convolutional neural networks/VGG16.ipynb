{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3619,
     "status": "ok",
     "timestamp": 1729269361711,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "JKAgOVUI-NAt",
    "outputId": "9a3fe7df-32a9-448b-eb55-37f887b5b160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/phamdinhtrunghieu/.cache/kagglehub/datasets/marquis03/cats-and-dogs/versions/2\n",
      "['.DS_Store', 'val.csv', 'train', 'train.csv', 'val']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"marquis03/cats-and-dogs\")\n",
    "# path = \"/Users/phamdinhtrunghieu/.cache/kagglehub/datasets/marquis03/cats-and-dogs/versions/2/train\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6666,
     "status": "ok",
     "timestamp": 1729269368373,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "9OYjRCScIS4G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "INPUTSIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "  resize_transform = transforms.Resize((INPUTSIZE,INPUTSIZE))\n",
    "\n",
    "  resized_img = resize_transform(img)\n",
    "\n",
    "  plt.figure(figsize=(8, 4))\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.imshow(img)\n",
    "  plt.title('Original Image')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.imshow(resized_img)\n",
    "  plt.title('Resized Image (224x224)')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10095,
     "status": "ok",
     "timestamp": 1729269378460,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "Y1uTwZGbInQt",
    "outputId": "a3e6b19a-509d-4eb0-a7c4-60d7873ea241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'cat']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = f\"{path}/train\"\n",
    "\n",
    "# labels = os.listdir(dataset_path)\n",
    "labels = [\"dog\", \"cat\"]\n",
    "print(labels)\n",
    "\n",
    "dataset = []\n",
    "\n",
    "count_label = 0\n",
    "\n",
    "number_of_label = len(labels)\n",
    "\n",
    "\n",
    "for label in labels:\n",
    "  data_path = f\"{dataset_path}/{label}\"\n",
    "  for example in os.listdir(data_path):\n",
    "    img = Image.open(f\"{data_path}/{example}\")\n",
    "    # show_image(img)\n",
    "    transform = transforms.Compose([\n",
    "                                    transforms.Resize((INPUTSIZE,INPUTSIZE)),\n",
    "                                    transforms.ToTensor()\n",
    "                                    ])\n",
    "    x = 1.0 * transform(img)\n",
    "    if (x.size()[0] != 3):\n",
    "      continue\n",
    "    y = torch.zeros(number_of_label)\n",
    "    y[count_label] = 1\n",
    "\n",
    "    # print(x,y)\n",
    "\n",
    "    dataset.append((x,y))\n",
    "\n",
    "  count_label += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1729269378462,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "DVTShB3AI6jd"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1729269379173,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "lEkjtZEqJnx9",
    "outputId": "de04de73-e0df-4bec-e3a9-03a61bb5727d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "max_w, max_h = 0, 0\n",
    "for x, y in dataset:\n",
    "  print(x.size())\n",
    "  print(y.size())\n",
    "  max_w = max(max_w, x.size()[1])\n",
    "  max_h = max(max_h, x.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1729269379173,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "rI_DMvbqJrLV",
    "outputId": "c6446750-fad6-46ef-d432-ef858248ac12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64\n"
     ]
    }
   ],
   "source": [
    "print(max_h, max_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729269379173,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "aKob6pd1KhjZ"
   },
   "outputs": [],
   "source": [
    "class ClassifierVGG16(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    tensor_size = INPUTSIZE\n",
    "    self.conv11 = nn.Conv2d(3, 64, 3, 1, 1) # 224 / 64\n",
    "    self.conv12 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "    self.maxpool1 = nn.MaxPool2d(2,2) # 112 / 32\n",
    "    tensor_size /= 2\n",
    "    \n",
    "    self.conv21 = nn.Conv2d(64, 128, 3, 1, 1) # 112 / 32\n",
    "    self.conv22 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "    self.maxpool2 = nn.MaxPool2d(2,2) # 56 / 16\n",
    "    tensor_size //= 2\n",
    "    \n",
    "    self.conv31 = nn.Conv2d(128, 256, 3, 1, 1) # 56 / 16\n",
    "    self.conv32 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "    self.conv33 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "    self.maxpool3 = nn.MaxPool2d(2,2) # 28 / 8\n",
    "    tensor_size //= 2\n",
    "    \n",
    "    self.conv41 = nn.Conv2d(256, 512, 3, 1, 1) # 28 / 8\n",
    "    self.conv42 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "    self.conv43 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "    self.maxpool4 = nn.MaxPool2d(2,2) # 14 / 4\n",
    "    tensor_size //= 2\n",
    "    \n",
    "    self.conv51 = nn.Conv2d(512, 512, 3, 1, 1) # 14\n",
    "    self.conv52 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "    self.conv53 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "    self.maxpool5 = nn.MaxPool2d(2,2) # 7 / 2\n",
    "    tensor_size //= 2\n",
    "    tensor_size = int(tensor_size)\n",
    "    \n",
    "    self.fc1 = nn.Linear(tensor_size * tensor_size * 512, 4096)\n",
    "    self.fc2 = nn.Linear(4096, 4096)\n",
    "    self.fc2 = nn.Linear(4096, number_of_label)\n",
    "    \n",
    "\n",
    "  def forward(self,x):\n",
    "    x = f.relu(self.conv11(x))\n",
    "    x = f.relu(self.conv12(x))\n",
    "    x = self.maxpool1(x)\n",
    "    # print(x.size())\n",
    "    \n",
    "    x = f.relu(self.conv21(x))\n",
    "    x = f.relu(self.conv22(x))\n",
    "    x = self.maxpool2(x)\n",
    "    # print(x.size())\n",
    "    \n",
    "    x = f.relu(self.conv31(x))\n",
    "    x = f.relu(self.conv32(x))\n",
    "    x = f.relu(self.conv33(x))\n",
    "    x = self.maxpool3(x)\n",
    "    # print(x.size())\n",
    "    \n",
    "    x = f.relu(self.conv41(x))\n",
    "    x = f.relu(self.conv42(x))\n",
    "    x = f.relu(self.conv43(x))\n",
    "    x = self.maxpool4(x)\n",
    "    # print(x.size())\n",
    "    \n",
    "    x = f.relu(self.conv51(x))\n",
    "    x = f.relu(self.conv52(x))\n",
    "    x = f.relu(self.conv53(x))\n",
    "    x = self.maxpool5(x)\n",
    "    # print(x.size())\n",
    "\n",
    "    x = torch.flatten(x)\n",
    "    \n",
    "    x = f.relu(self.fc1(x))\n",
    "    x = f.relu(self.fc2(x))\n",
    "\n",
    "    return f.softmax(x, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729269379173,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "StYAEZFgcOiR"
   },
   "outputs": [],
   "source": [
    "def get_label(y):\n",
    "  res = 0\n",
    "\n",
    "  for id in range(number_of_label):\n",
    "    if y[res].item() < y[id].item():\n",
    "      res = id\n",
    "\n",
    "\n",
    "\n",
    "  return labels[res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 144303,
     "status": "error",
     "timestamp": 1729269523814,
     "user": {
      "displayName": "Hiếu Phạm Đình Trung",
      "userId": "04488184884666032393"
     },
     "user_tz": -420
    },
    "id": "wcGpl0P6SoTQ",
    "outputId": "0938091b-ae60-4af5-8e6a-5db62d0d3d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5019, 0.4981], grad_fn=<SoftmaxBackward0>)\n",
      "=====epoch:0=====\n",
      "Epoch 0 - 0: dog - dog || Loss: 0.24812953174114227\n",
      "tensor([1., 0.]) tensor([0.5019, 0.4981], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 1: dog - dog || Loss: 0.24793076515197754\n",
      "tensor([1., 0.]) tensor([0.5021, 0.4979], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 2: cat - dog || Loss: 0.25245898962020874\n",
      "tensor([0., 1.]) tensor([0.5025, 0.4975], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 3: cat - dog || Loss: 0.2525995671749115\n",
      "tensor([0., 1.]) tensor([0.5026, 0.4974], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 4: cat - dog || Loss: 0.2525235712528229\n",
      "tensor([0., 1.]) tensor([0.5025, 0.4975], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 5: cat - dog || Loss: 0.252252995967865\n",
      "tensor([0., 1.]) tensor([0.5022, 0.4978], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 6: cat - dog || Loss: 0.25180739164352417\n",
      "tensor([0., 1.]) tensor([0.5018, 0.4982], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 7: dog - dog || Loss: 0.24879774451255798\n",
      "tensor([1., 0.]) tensor([0.5012, 0.4988], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 8: cat - dog || Loss: 0.2508641481399536\n",
      "tensor([0., 1.]) tensor([0.5009, 0.4991], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 9: dog - dog || Loss: 0.24964413046836853\n",
      "tensor([1., 0.]) tensor([0.5004, 0.4996], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 10: dog - dog || Loss: 0.2499002367258072\n",
      "tensor([1., 0.]) tensor([0.5001, 0.4999], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 11: cat - dog || Loss: 0.2500697672367096\n",
      "tensor([0., 1.]) tensor([0.5001, 0.4999], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 12: cat - cat || Loss: 0.24984218180179596\n",
      "tensor([0., 1.]) tensor([0.4998, 0.5002], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 13: dog - cat || Loss: 0.2505636215209961\n",
      "tensor([1., 0.]) tensor([0.4994, 0.5006], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 14: dog - cat || Loss: 0.2507278323173523\n",
      "tensor([1., 0.]) tensor([0.4993, 0.5007], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 15: cat - cat || Loss: 0.24932625889778137\n",
      "tensor([0., 1.]) tensor([0.4993, 0.5007], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 16: cat - cat || Loss: 0.24917419254779816\n",
      "tensor([0., 1.]) tensor([0.4992, 0.5008], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 17: dog - cat || Loss: 0.2511651813983917\n",
      "tensor([1., 0.]) tensor([0.4988, 0.5012], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 18: dog - cat || Loss: 0.2512681186199188\n",
      "tensor([1., 0.]) tensor([0.4987, 0.5013], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 19: cat - cat || Loss: 0.24884334206581116\n",
      "tensor([0., 1.]) tensor([0.4988, 0.5012], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 20: dog - cat || Loss: 0.25126177072525024\n",
      "tensor([1., 0.]) tensor([0.4987, 0.5013], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 21: dog - cat || Loss: 0.25115257501602173\n",
      "tensor([1., 0.]) tensor([0.4988, 0.5012], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 22: cat - cat || Loss: 0.2491484433412552\n",
      "tensor([0., 1.]) tensor([0.4991, 0.5009], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 23: dog - cat || Loss: 0.25078409910202026\n",
      "tensor([1., 0.]) tensor([0.4992, 0.5008], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 24: cat - cat || Loss: 0.24947945773601532\n",
      "tensor([0., 1.]) tensor([0.4995, 0.5005], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 25: dog - cat || Loss: 0.25048476457595825\n",
      "tensor([1., 0.]) tensor([0.4995, 0.5005], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 26: dog - cat || Loss: 0.25025129318237305\n",
      "tensor([1., 0.]) tensor([0.4997, 0.5003], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 27: cat - dog || Loss: 0.25015929341316223\n",
      "tensor([0., 1.]) tensor([0.5002, 0.4998], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 28: cat - dog || Loss: 0.2503282129764557\n",
      "tensor([0., 1.]) tensor([0.5003, 0.4997], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 29: cat - dog || Loss: 0.2502796947956085\n",
      "tensor([0., 1.]) tensor([0.5003, 0.4997], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 30: dog - dog || Loss: 0.2499643713235855\n",
      "tensor([1., 0.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 31: cat - dog || Loss: 0.25001615285873413\n",
      "tensor([0., 1.]) tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 32: cat - cat || Loss: 0.24979840219020844\n",
      "tensor([0., 1.]) tensor([0.4998, 0.5002], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 33: dog - cat || Loss: 0.25059834122657776\n",
      "tensor([1., 0.]) tensor([0.4994, 0.5006], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 34: cat - cat || Loss: 0.24924662709236145\n",
      "tensor([0., 1.]) tensor([0.4992, 0.5008], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 35: dog - cat || Loss: 0.2510959506034851\n",
      "tensor([1., 0.]) tensor([0.4989, 0.5011], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 36: dog - cat || Loss: 0.2512016296386719\n",
      "tensor([1., 0.]) tensor([0.4988, 0.5012], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 37: cat - cat || Loss: 0.2489067018032074\n",
      "tensor([0., 1.]) tensor([0.4989, 0.5011], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 38: cat - cat || Loss: 0.24880240857601166\n",
      "tensor([0., 1.]) tensor([0.4988, 0.5012], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 39: dog - cat || Loss: 0.2514955997467041\n",
      "tensor([1., 0.]) tensor([0.4985, 0.5015], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 40: cat - cat || Loss: 0.2484453171491623\n",
      "tensor([0., 1.]) tensor([0.4984, 0.5016], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 41: cat - cat || Loss: 0.24818910658359528\n",
      "tensor([0., 1.]) tensor([0.4982, 0.5018], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 42: cat - cat || Loss: 0.24775958061218262\n",
      "tensor([0., 1.]) tensor([0.4978, 0.5022], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 43: cat - cat || Loss: 0.24717500805854797\n",
      "tensor([0., 1.]) tensor([0.4972, 0.5028], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 44: cat - cat || Loss: 0.24645160138607025\n",
      "tensor([0., 1.]) tensor([0.4964, 0.5036], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 45: dog - cat || Loss: 0.25418877601623535\n",
      "tensor([1., 0.]) tensor([0.4958, 0.5042], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 46: dog - cat || Loss: 0.25447413325309753\n",
      "tensor([1., 0.]) tensor([0.4955, 0.5045], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 47: cat - cat || Loss: 0.24541297554969788\n",
      "tensor([0., 1.]) tensor([0.4954, 0.5046], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 48: dog - cat || Loss: 0.25486916303634644\n",
      "tensor([1., 0.]) tensor([0.4952, 0.5048], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 49: cat - cat || Loss: 0.24506619572639465\n",
      "tensor([0., 1.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 50: cat - cat || Loss: 0.244867742061615\n",
      "tensor([0., 1.]) tensor([0.4948, 0.5052], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 51: dog - cat || Loss: 0.2554674744606018\n",
      "tensor([1., 0.]) tensor([0.4946, 0.5054], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 52: dog - cat || Loss: 0.25561952590942383\n",
      "tensor([1., 0.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 53: dog - cat || Loss: 0.25565415620803833\n",
      "tensor([1., 0.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 54: dog - cat || Loss: 0.2555834949016571\n",
      "tensor([1., 0.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 55: cat - cat || Loss: 0.24464036524295807\n",
      "tensor([0., 1.]) tensor([0.4946, 0.5054], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 56: dog - cat || Loss: 0.2553684115409851\n",
      "tensor([1., 0.]) tensor([0.4947, 0.5053], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 57: cat - cat || Loss: 0.24483181536197662\n",
      "tensor([0., 1.]) tensor([0.4948, 0.5052], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 58: dog - cat || Loss: 0.25519034266471863\n",
      "tensor([1., 0.]) tensor([0.4948, 0.5052], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 59: dog - cat || Loss: 0.25505977869033813\n",
      "tensor([1., 0.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 60: cat - cat || Loss: 0.24520578980445862\n",
      "tensor([0., 1.]) tensor([0.4952, 0.5048], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 61: dog - cat || Loss: 0.2547432482242584\n",
      "tensor([1., 0.]) tensor([0.4953, 0.5047], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 62: dog - cat || Loss: 0.2545539438724518\n",
      "tensor([1., 0.]) tensor([0.4955, 0.5045], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 63: dog - cat || Loss: 0.25428205728530884\n",
      "tensor([1., 0.]) tensor([0.4957, 0.5043], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 64: dog - cat || Loss: 0.2539362609386444\n",
      "tensor([1., 0.]) tensor([0.4961, 0.5039], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 65: cat - cat || Loss: 0.24650080502033234\n",
      "tensor([0., 1.]) tensor([0.4965, 0.5035], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 66: dog - cat || Loss: 0.25325286388397217\n",
      "tensor([1., 0.]) tensor([0.4968, 0.5032], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 67: cat - cat || Loss: 0.24710877239704132\n",
      "tensor([0., 1.]) tensor([0.4971, 0.5029], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 68: dog - cat || Loss: 0.25269758701324463\n",
      "tensor([1., 0.]) tensor([0.4973, 0.5027], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 69: dog - cat || Loss: 0.2524075508117676\n",
      "tensor([1., 0.]) tensor([0.4976, 0.5024], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 70: cat - cat || Loss: 0.2479623258113861\n",
      "tensor([0., 1.]) tensor([0.4980, 0.5020], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 71: dog - cat || Loss: 0.2518205940723419\n",
      "tensor([1., 0.]) tensor([0.4982, 0.5018], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 72: cat - cat || Loss: 0.24848714470863342\n",
      "tensor([0., 1.]) tensor([0.4985, 0.5015], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 73: cat - cat || Loss: 0.2486591935157776\n",
      "tensor([0., 1.]) tensor([0.4987, 0.5013], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 74: cat - cat || Loss: 0.24871498346328735\n",
      "tensor([0., 1.]) tensor([0.4987, 0.5013], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 75: cat - cat || Loss: 0.24866603314876556\n",
      "tensor([0., 1.]) tensor([0.4987, 0.5013], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 76: dog - cat || Loss: 0.2514815628528595\n",
      "tensor([1., 0.]) tensor([0.4985, 0.5015], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 77: cat - cat || Loss: 0.2484935224056244\n",
      "tensor([0., 1.]) tensor([0.4985, 0.5015], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 78: cat - cat || Loss: 0.24836811423301697\n",
      "tensor([0., 1.]) tensor([0.4984, 0.5016], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 79: cat - cat || Loss: 0.24815630912780762\n",
      "tensor([0., 1.]) tensor([0.4982, 0.5018], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 80: dog - cat || Loss: 0.2521423399448395\n",
      "tensor([1., 0.]) tensor([0.4979, 0.5021], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 81: cat - cat || Loss: 0.24770615994930267\n",
      "tensor([0., 1.]) tensor([0.4977, 0.5023], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 82: dog - cat || Loss: 0.2525501549243927\n",
      "tensor([1., 0.]) tensor([0.4975, 0.5025], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 83: dog - cat || Loss: 0.2526707053184509\n",
      "tensor([1., 0.]) tensor([0.4973, 0.5027], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 84: dog - cat || Loss: 0.25267836451530457\n",
      "tensor([1., 0.]) tensor([0.4973, 0.5027], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 85: cat - cat || Loss: 0.24742862582206726\n",
      "tensor([0., 1.]) tensor([0.4974, 0.5026], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 86: dog - cat || Loss: 0.25259992480278015\n",
      "tensor([1., 0.]) tensor([0.4974, 0.5026], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 87: dog - cat || Loss: 0.25251293182373047\n",
      "tensor([1., 0.]) tensor([0.4975, 0.5025], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 88: dog - cat || Loss: 0.2523338794708252\n",
      "tensor([1., 0.]) tensor([0.4977, 0.5023], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 89: cat - cat || Loss: 0.2479361593723297\n",
      "tensor([0., 1.]) tensor([0.4979, 0.5021], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 90: cat - cat || Loss: 0.24807065725326538\n",
      "tensor([0., 1.]) tensor([0.4981, 0.5019], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 91: dog - cat || Loss: 0.251914381980896\n",
      "tensor([1., 0.]) tensor([0.4981, 0.5019], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 92: cat - cat || Loss: 0.24821266531944275\n",
      "tensor([0., 1.]) tensor([0.4982, 0.5018], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 93: cat - cat || Loss: 0.24822133779525757\n",
      "tensor([0., 1.]) tensor([0.4982, 0.5018], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 94: cat - cat || Loss: 0.24813027679920197\n",
      "tensor([0., 1.]) tensor([0.4981, 0.5019], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 95: dog - cat || Loss: 0.2520590126514435\n",
      "tensor([1., 0.]) tensor([0.4979, 0.5021], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 96: dog - cat || Loss: 0.2521227300167084\n",
      "tensor([1., 0.]) tensor([0.4979, 0.5021], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 97: dog - cat || Loss: 0.25207939743995667\n",
      "tensor([1., 0.]) tensor([0.4979, 0.5021], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 98: cat - cat || Loss: 0.24806755781173706\n",
      "tensor([0., 1.]) tensor([0.4981, 0.5019], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 99: dog - cat || Loss: 0.25191420316696167\n",
      "tensor([1., 0.]) tensor([0.4981, 0.5019], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 100: cat - cat || Loss: 0.248215913772583\n",
      "tensor([0., 1.]) tensor([0.4982, 0.5018], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 101: cat - cat || Loss: 0.24822740256786346\n",
      "tensor([0., 1.]) tensor([0.4982, 0.5018], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 102: cat - cat || Loss: 0.24813877046108246\n",
      "tensor([0., 1.]) tensor([0.4981, 0.5019], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 103: dog - cat || Loss: 0.25204816460609436\n",
      "tensor([1., 0.]) tensor([0.4980, 0.5020], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 104: dog - cat || Loss: 0.2521098256111145\n",
      "tensor([1., 0.]) tensor([0.4979, 0.5021], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 105: cat - cat || Loss: 0.24794380366802216\n",
      "tensor([0., 1.]) tensor([0.4979, 0.5021], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 106: cat - cat || Loss: 0.24788519740104675\n",
      "tensor([0., 1.]) tensor([0.4979, 0.5021], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 107: dog - cat || Loss: 0.2522766888141632\n",
      "tensor([1., 0.]) tensor([0.4977, 0.5023], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 108: cat - cat || Loss: 0.24769695103168488\n",
      "tensor([0., 1.]) tensor([0.4977, 0.5023], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 109: cat - cat || Loss: 0.2475651651620865\n",
      "tensor([0., 1.]) tensor([0.4976, 0.5024], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 110: dog - cat || Loss: 0.2526662051677704\n",
      "tensor([1., 0.]) tensor([0.4973, 0.5027], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 111: cat - cat || Loss: 0.24725213646888733\n",
      "tensor([0., 1.]) tensor([0.4972, 0.5028], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 112: dog - cat || Loss: 0.25294995307922363\n",
      "tensor([1., 0.]) tensor([0.4971, 0.5029], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 113: cat - cat || Loss: 0.24700072407722473\n",
      "tensor([0., 1.]) tensor([0.4970, 0.5030], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 114: cat - cat || Loss: 0.2468423694372177\n",
      "tensor([0., 1.]) tensor([0.4968, 0.5032], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 115: cat - cat || Loss: 0.24660155177116394\n",
      "tensor([0., 1.]) tensor([0.4966, 0.5034], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 116: dog - cat || Loss: 0.25374114513397217\n",
      "tensor([1., 0.]) tensor([0.4963, 0.5037], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 117: cat - cat || Loss: 0.24610291421413422\n",
      "tensor([0., 1.]) tensor([0.4961, 0.5039], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 118: cat - cat || Loss: 0.24583961069583893\n",
      "tensor([0., 1.]) tensor([0.4958, 0.5042], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 119: dog - cat || Loss: 0.25453606247901917\n",
      "tensor([1., 0.]) tensor([0.4955, 0.5045], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 120: cat - cat || Loss: 0.2453031986951828\n",
      "tensor([0., 1.]) tensor([0.4953, 0.5047], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 121: cat - cat || Loss: 0.24502408504486084\n",
      "tensor([0., 1.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 122: cat - cat || Loss: 0.244675412774086\n",
      "tensor([0., 1.]) tensor([0.4946, 0.5054], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 123: cat - cat || Loss: 0.2442644089460373\n",
      "tensor([0., 1.]) tensor([0.4942, 0.5058], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 124: cat - cat || Loss: 0.2437973916530609\n",
      "tensor([0., 1.]) tensor([0.4938, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 125: cat - cat || Loss: 0.24328044056892395\n",
      "tensor([0., 1.]) tensor([0.4932, 0.5068], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 126: cat - cat || Loss: 0.24271857738494873\n",
      "tensor([0., 1.]) tensor([0.4927, 0.5073], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 127: dog - cat || Loss: 0.2580093443393707\n",
      "tensor([1., 0.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 128: cat - cat || Loss: 0.24167557060718536\n",
      "tensor([0., 1.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 129: cat - cat || Loss: 0.24118240177631378\n",
      "tensor([0., 1.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 130: dog - cat || Loss: 0.25953561067581177\n",
      "tensor([1., 0.]) tensor([0.4906, 0.5094], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 131: dog - cat || Loss: 0.2599361538887024\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 132: cat - cat || Loss: 0.24001052975654602\n",
      "tensor([0., 1.]) tensor([0.4899, 0.5101], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 133: dog - cat || Loss: 0.2605242431163788\n",
      "tensor([1., 0.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 134: dog - cat || Loss: 0.260718435049057\n",
      "tensor([1., 0.]) tensor([0.4894, 0.5106], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 135: cat - cat || Loss: 0.23943865299224854\n",
      "tensor([0., 1.]) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 136: dog - cat || Loss: 0.2609526813030243\n",
      "tensor([1., 0.]) tensor([0.4892, 0.5108], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 137: dog - cat || Loss: 0.2609958052635193\n",
      "tensor([1., 0.]) tensor([0.4891, 0.5109], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 138: dog - cat || Loss: 0.26093053817749023\n",
      "tensor([1., 0.]) tensor([0.4892, 0.5108], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 139: dog - cat || Loss: 0.26076769828796387\n",
      "tensor([1., 0.]) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 140: cat - cat || Loss: 0.2396993488073349\n",
      "tensor([0., 1.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 141: dog - cat || Loss: 0.2603917717933655\n",
      "tensor([1., 0.]) tensor([0.4897, 0.5103], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 142: dog - cat || Loss: 0.2601749300956726\n",
      "tensor([1., 0.]) tensor([0.4899, 0.5101], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 143: cat - cat || Loss: 0.24031516909599304\n",
      "tensor([0., 1.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 144: dog - cat || Loss: 0.25970709323883057\n",
      "tensor([1., 0.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 145: cat - cat || Loss: 0.2407238781452179\n",
      "tensor([0., 1.]) tensor([0.4906, 0.5094], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 146: cat - cat || Loss: 0.24084949493408203\n",
      "tensor([0., 1.]) tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 147: cat - cat || Loss: 0.24086658656597137\n",
      "tensor([0., 1.]) tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 148: dog - cat || Loss: 0.2593870162963867\n",
      "tensor([1., 0.]) tensor([0.4907, 0.5093], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 149: cat - cat || Loss: 0.24081292748451233\n",
      "tensor([0., 1.]) tensor([0.4907, 0.5093], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 150: cat - cat || Loss: 0.2407412976026535\n",
      "tensor([0., 1.]) tensor([0.4907, 0.5093], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 151: dog - cat || Loss: 0.25959986448287964\n",
      "tensor([1., 0.]) tensor([0.4905, 0.5095], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 152: cat - cat || Loss: 0.24053621292114258\n",
      "tensor([0., 1.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 153: cat - cat || Loss: 0.24040012061595917\n",
      "tensor([0., 1.]) tensor([0.4903, 0.5097], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 154: dog - cat || Loss: 0.26001474261283875\n",
      "tensor([1., 0.]) tensor([0.4901, 0.5099], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 155: cat - cat || Loss: 0.24008511006832123\n",
      "tensor([0., 1.]) tensor([0.4900, 0.5100], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 156: dog - cat || Loss: 0.26030561327934265\n",
      "tensor([1., 0.]) tensor([0.4898, 0.5102], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 157: cat - cat || Loss: 0.2398376613855362\n",
      "tensor([0., 1.]) tensor([0.4897, 0.5103], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 158: cat - cat || Loss: 0.23968367278575897\n",
      "tensor([0., 1.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 159: dog - cat || Loss: 0.2607775926589966\n",
      "tensor([1., 0.]) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 160: dog - cat || Loss: 0.26089340448379517\n",
      "tensor([1., 0.]) tensor([0.4892, 0.5108], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 161: cat - cat || Loss: 0.2393387109041214\n",
      "tensor([0., 1.]) tensor([0.4892, 0.5108], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 162: cat - cat || Loss: 0.23924316465854645\n",
      "tensor([0., 1.]) tensor([0.4891, 0.5109], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 163: cat - cat || Loss: 0.23906183242797852\n",
      "tensor([0., 1.]) tensor([0.4889, 0.5111], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 164: cat - cat || Loss: 0.23880352079868317\n",
      "tensor([0., 1.]) tensor([0.4887, 0.5113], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 165: dog - cat || Loss: 0.26179590821266174\n",
      "tensor([1., 0.]) tensor([0.4883, 0.5117], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 166: dog - cat || Loss: 0.2620002031326294\n",
      "tensor([1., 0.]) tensor([0.4881, 0.5119], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 167: dog - cat || Loss: 0.26207977533340454\n",
      "tensor([1., 0.]) tensor([0.4881, 0.5119], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 168: cat - cat || Loss: 0.23823639750480652\n",
      "tensor([0., 1.]) tensor([0.4881, 0.5119], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 169: cat - cat || Loss: 0.23816964030265808\n",
      "tensor([0., 1.]) tensor([0.4880, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 170: cat - cat || Loss: 0.23801471292972565\n",
      "tensor([0., 1.]) tensor([0.4879, 0.5121], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 171: cat - cat || Loss: 0.2377805858850479\n",
      "tensor([0., 1.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 172: dog - cat || Loss: 0.26284652948379517\n",
      "tensor([1., 0.]) tensor([0.4873, 0.5127], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 173: dog - cat || Loss: 0.2630307674407959\n",
      "tensor([1., 0.]) tensor([0.4871, 0.5129], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 174: dog - cat || Loss: 0.2630918622016907\n",
      "tensor([1., 0.]) tensor([0.4871, 0.5129], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 175: cat - cat || Loss: 0.2372894138097763\n",
      "tensor([0., 1.]) tensor([0.4871, 0.5129], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 176: cat - cat || Loss: 0.23723755776882172\n",
      "tensor([0., 1.]) tensor([0.4871, 0.5129], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 177: dog - cat || Loss: 0.2632456123828888\n",
      "tensor([1., 0.]) tensor([0.4869, 0.5131], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 178: cat - cat || Loss: 0.23706869781017303\n",
      "tensor([0., 1.]) tensor([0.4869, 0.5131], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 179: cat - cat || Loss: 0.23694951832294464\n",
      "tensor([0., 1.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 180: cat - cat || Loss: 0.2367478907108307\n",
      "tensor([0., 1.]) tensor([0.4866, 0.5134], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 181: dog - cat || Loss: 0.26390397548675537\n",
      "tensor([1., 0.]) tensor([0.4863, 0.5137], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 182: cat - cat || Loss: 0.2363237738609314\n",
      "tensor([0., 1.]) tensor([0.4861, 0.5139], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 183: cat - cat || Loss: 0.2360960841178894\n",
      "tensor([0., 1.]) tensor([0.4859, 0.5141], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 184: dog - cat || Loss: 0.26461800932884216\n",
      "tensor([1., 0.]) tensor([0.4856, 0.5144], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 185: cat - cat || Loss: 0.23562803864479065\n",
      "tensor([0., 1.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 186: dog - cat || Loss: 0.26505836844444275\n",
      "tensor([1., 0.]) tensor([0.4852, 0.5148], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 187: dog - cat || Loss: 0.26518765091896057\n",
      "tensor([1., 0.]) tensor([0.4850, 0.5150], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 188: cat - cat || Loss: 0.23525002598762512\n",
      "tensor([0., 1.]) tensor([0.4850, 0.5150], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 189: dog - cat || Loss: 0.265307754278183\n",
      "tensor([1., 0.]) tensor([0.4849, 0.5151], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 190: cat - cat || Loss: 0.23515406250953674\n",
      "tensor([0., 1.]) tensor([0.4849, 0.5151], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 191: dog - cat || Loss: 0.2653932273387909\n",
      "tensor([1., 0.]) tensor([0.4848, 0.5152], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 192: cat - cat || Loss: 0.23508746922016144\n",
      "tensor([0., 1.]) tensor([0.4849, 0.5151], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 193: cat - cat || Loss: 0.23501257598400116\n",
      "tensor([0., 1.]) tensor([0.4848, 0.5152], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 194: dog - cat || Loss: 0.2656218409538269\n",
      "tensor([1., 0.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 195: dog - cat || Loss: 0.2656700611114502\n",
      "tensor([1., 0.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 196: cat - cat || Loss: 0.23486483097076416\n",
      "tensor([0., 1.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 197: dog - cat || Loss: 0.2656511068344116\n",
      "tensor([1., 0.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 198: cat - cat || Loss: 0.23488688468933105\n",
      "tensor([0., 1.]) tensor([0.4847, 0.5153], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 199: cat - cat || Loss: 0.23484988510608673\n",
      "tensor([0., 1.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 200: cat - cat || Loss: 0.2347230464220047\n",
      "tensor([0., 1.]) tensor([0.4845, 0.5155], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 201: cat - cat || Loss: 0.2345154881477356\n",
      "tensor([0., 1.]) tensor([0.4843, 0.5157], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 202: dog - cat || Loss: 0.2662777900695801\n",
      "tensor([1., 0.]) tensor([0.4840, 0.5160], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 203: cat - cat || Loss: 0.2340831309556961\n",
      "tensor([0., 1.]) tensor([0.4838, 0.5162], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 204: dog - cat || Loss: 0.26668620109558105\n",
      "tensor([1., 0.]) tensor([0.4836, 0.5164], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 205: dog - cat || Loss: 0.2668014168739319\n",
      "tensor([1., 0.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 206: cat - cat || Loss: 0.23374734818935394\n",
      "tensor([0., 1.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 207: dog - cat || Loss: 0.26689592003822327\n",
      "tensor([1., 0.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 208: cat - cat || Loss: 0.23367418348789215\n",
      "tensor([0., 1.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 209: dog - cat || Loss: 0.2669595181941986\n",
      "tensor([1., 0.]) tensor([0.4833, 0.5167], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 210: dog - cat || Loss: 0.26692748069763184\n",
      "tensor([1., 0.]) tensor([0.4833, 0.5167], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 211: cat - cat || Loss: 0.23375345766544342\n",
      "tensor([0., 1.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 212: dog - cat || Loss: 0.26677027344703674\n",
      "tensor([1., 0.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 213: dog - cat || Loss: 0.26664412021636963\n",
      "tensor([1., 0.]) tensor([0.4836, 0.5164], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 214: cat - cat || Loss: 0.2340979427099228\n",
      "tensor([0., 1.]) tensor([0.4838, 0.5162], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 215: dog - cat || Loss: 0.266326367855072\n",
      "tensor([1., 0.]) tensor([0.4839, 0.5161], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 216: cat - cat || Loss: 0.2343723177909851\n",
      "tensor([0., 1.]) tensor([0.4841, 0.5159], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 217: cat - cat || Loss: 0.23444309830665588\n",
      "tensor([0., 1.]) tensor([0.4842, 0.5158], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 218: dog - cat || Loss: 0.26608818769454956\n",
      "tensor([1., 0.]) tensor([0.4842, 0.5158], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 219: dog - cat || Loss: 0.2660106420516968\n",
      "tensor([1., 0.]) tensor([0.4842, 0.5158], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 220: cat - cat || Loss: 0.23465125262737274\n",
      "tensor([0., 1.]) tensor([0.4844, 0.5156], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 221: dog - cat || Loss: 0.2657763361930847\n",
      "tensor([1., 0.]) tensor([0.4845, 0.5155], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 222: cat - cat || Loss: 0.23485535383224487\n",
      "tensor([0., 1.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 223: dog - cat || Loss: 0.2655746042728424\n",
      "tensor([1., 0.]) tensor([0.4847, 0.5153], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 224: cat - cat || Loss: 0.23503203690052032\n",
      "tensor([0., 1.]) tensor([0.4848, 0.5152], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 225: cat - cat || Loss: 0.2350609451532364\n",
      "tensor([0., 1.]) tensor([0.4848, 0.5152], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 226: dog - cat || Loss: 0.2654710114002228\n",
      "tensor([1., 0.]) tensor([0.4848, 0.5152], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 227: dog - cat || Loss: 0.26542994379997253\n",
      "tensor([1., 0.]) tensor([0.4848, 0.5152], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 228: dog - cat || Loss: 0.26528748869895935\n",
      "tensor([1., 0.]) tensor([0.4849, 0.5151], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 229: dog - cat || Loss: 0.26505357027053833\n",
      "tensor([1., 0.]) tensor([0.4852, 0.5148], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 230: cat - cat || Loss: 0.23568424582481384\n",
      "tensor([0., 1.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 231: cat - cat || Loss: 0.23585858941078186\n",
      "tensor([0., 1.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 232: dog - cat || Loss: 0.2644863724708557\n",
      "tensor([1., 0.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 233: cat - cat || Loss: 0.23607778549194336\n",
      "tensor([0., 1.]) tensor([0.4859, 0.5141], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 234: cat - cat || Loss: 0.2361244112253189\n",
      "tensor([0., 1.]) tensor([0.4859, 0.5141], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 235: cat - cat || Loss: 0.23607237637043\n",
      "tensor([0., 1.]) tensor([0.4859, 0.5141], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 236: cat - cat || Loss: 0.23593169450759888\n",
      "tensor([0., 1.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 237: dog - cat || Loss: 0.2647092640399933\n",
      "tensor([1., 0.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 238: dog - cat || Loss: 0.2648141384124756\n",
      "tensor([1., 0.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 239: dog - cat || Loss: 0.2648032605648041\n",
      "tensor([1., 0.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 240: dog - cat || Loss: 0.2646879255771637\n",
      "tensor([1., 0.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 241: dog - cat || Loss: 0.2644789218902588\n",
      "tensor([1., 0.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 242: cat - cat || Loss: 0.23620577156543732\n",
      "tensor([0., 1.]) tensor([0.4860, 0.5140], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 243: cat - cat || Loss: 0.23636135458946228\n",
      "tensor([0., 1.]) tensor([0.4862, 0.5138], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 244: cat - cat || Loss: 0.23640716075897217\n",
      "tensor([0., 1.]) tensor([0.4862, 0.5138], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 245: cat - cat || Loss: 0.23635441064834595\n",
      "tensor([0., 1.]) tensor([0.4862, 0.5138], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 246: cat - cat || Loss: 0.23621296882629395\n",
      "tensor([0., 1.]) tensor([0.4860, 0.5140], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 247: cat - cat || Loss: 0.2359916716814041\n",
      "tensor([0., 1.]) tensor([0.4858, 0.5142], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 248: dog - cat || Loss: 0.26472240686416626\n",
      "tensor([1., 0.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 249: dog - cat || Loss: 0.26489630341529846\n",
      "tensor([1., 0.]) tensor([0.4853, 0.5147], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 250: dog - cat || Loss: 0.2649475634098053\n",
      "tensor([1., 0.]) tensor([0.4853, 0.5147], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 251: cat - cat || Loss: 0.23554234206676483\n",
      "tensor([0., 1.]) tensor([0.4853, 0.5147], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 252: cat - cat || Loss: 0.23549897968769073\n",
      "tensor([0., 1.]) tensor([0.4853, 0.5147], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 253: dog - cat || Loss: 0.26507502794265747\n",
      "tensor([1., 0.]) tensor([0.4851, 0.5149], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 254: cat - cat || Loss: 0.2353462129831314\n",
      "tensor([0., 1.]) tensor([0.4851, 0.5149], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 255: dog - cat || Loss: 0.26521483063697815\n",
      "tensor([1., 0.]) tensor([0.4850, 0.5150], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 256: dog - cat || Loss: 0.26521602272987366\n",
      "tensor([1., 0.]) tensor([0.4850, 0.5150], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 257: dog - cat || Loss: 0.26511162519454956\n",
      "tensor([1., 0.]) tensor([0.4851, 0.5149], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 258: dog - cat || Loss: 0.26491206884384155\n",
      "tensor([1., 0.]) tensor([0.4853, 0.5147], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 259: cat - cat || Loss: 0.23578859865665436\n",
      "tensor([0., 1.]) tensor([0.4856, 0.5144], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 260: cat - cat || Loss: 0.23593670129776\n",
      "tensor([0., 1.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 261: cat - cat || Loss: 0.23597611486911774\n",
      "tensor([0., 1.]) tensor([0.4858, 0.5142], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 262: cat - cat || Loss: 0.23591773211956024\n",
      "tensor([0., 1.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 263: dog - cat || Loss: 0.2646455764770508\n",
      "tensor([1., 0.]) tensor([0.4856, 0.5144], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 264: cat - cat || Loss: 0.23573896288871765\n",
      "tensor([0., 1.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 265: dog - cat || Loss: 0.2648099958896637\n",
      "tensor([1., 0.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 266: dog - cat || Loss: 0.26482197642326355\n",
      "tensor([1., 0.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 267: dog - cat || Loss: 0.26472732424736023\n",
      "tensor([1., 0.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 268: cat - cat || Loss: 0.2358739823102951\n",
      "tensor([0., 1.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 269: dog - cat || Loss: 0.26446476578712463\n",
      "tensor([1., 0.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 270: cat - cat || Loss: 0.2361026406288147\n",
      "tensor([0., 1.]) tensor([0.4859, 0.5141], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 271: dog - cat || Loss: 0.26424115896224976\n",
      "tensor([1., 0.]) tensor([0.4860, 0.5140], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 272: cat - cat || Loss: 0.2362983077764511\n",
      "tensor([0., 1.]) tensor([0.4861, 0.5139], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 273: dog - cat || Loss: 0.264049232006073\n",
      "tensor([1., 0.]) tensor([0.4861, 0.5139], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 274: dog - cat || Loss: 0.2639094293117523\n",
      "tensor([1., 0.]) tensor([0.4863, 0.5137], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 275: dog - cat || Loss: 0.2636786997318268\n",
      "tensor([1., 0.]) tensor([0.4865, 0.5135], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 276: cat - cat || Loss: 0.23698186874389648\n",
      "tensor([0., 1.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 277: dog - cat || Loss: 0.263184517621994\n",
      "tensor([1., 0.]) tensor([0.4870, 0.5130], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 278: dog - cat || Loss: 0.2629163861274719\n",
      "tensor([1., 0.]) tensor([0.4872, 0.5128], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 279: dog - cat || Loss: 0.26257064938545227\n",
      "tensor([1., 0.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 280: cat - cat || Loss: 0.2381332814693451\n",
      "tensor([0., 1.]) tensor([0.4880, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 281: cat - cat || Loss: 0.23839479684829712\n",
      "tensor([0., 1.]) tensor([0.4883, 0.5117], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 282: dog - cat || Loss: 0.2617337107658386\n",
      "tensor([1., 0.]) tensor([0.4884, 0.5116], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 283: dog - cat || Loss: 0.26149702072143555\n",
      "tensor([1., 0.]) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 284: dog - cat || Loss: 0.26118019223213196\n",
      "tensor([1., 0.]) tensor([0.4889, 0.5111], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 285: dog - cat || Loss: 0.26079124212265015\n",
      "tensor([1., 0.]) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 286: dog - cat || Loss: 0.2603376507759094\n",
      "tensor([1., 0.]) tensor([0.4898, 0.5102], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 287: dog - cat || Loss: 0.2598262429237366\n",
      "tensor([1., 0.]) tensor([0.4903, 0.5097], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 288: cat - cat || Loss: 0.24090541899204254\n",
      "tensor([0., 1.]) tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 289: dog - cat || Loss: 0.2588561177253723\n",
      "tensor([1., 0.]) tensor([0.4912, 0.5088], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 290: dog - cat || Loss: 0.2583872973918915\n",
      "tensor([1., 0.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 291: cat - cat || Loss: 0.24225890636444092\n",
      "tensor([0., 1.]) tensor([0.4922, 0.5078], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 292: dog - cat || Loss: 0.25749072432518005\n",
      "tensor([1., 0.]) tensor([0.4926, 0.5074], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 293: cat - cat || Loss: 0.24304449558258057\n",
      "tensor([0., 1.]) tensor([0.4930, 0.5070], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 294: dog - cat || Loss: 0.2567601203918457\n",
      "tensor([1., 0.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 295: cat - cat || Loss: 0.24368691444396973\n",
      "tensor([0., 1.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 296: dog - cat || Loss: 0.2561638355255127\n",
      "tensor([1., 0.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 297: cat - cat || Loss: 0.2442125827074051\n",
      "tensor([0., 1.]) tensor([0.4942, 0.5058], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 298: cat - cat || Loss: 0.24438685178756714\n",
      "tensor([0., 1.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 299: cat - cat || Loss: 0.24444648623466492\n",
      "tensor([0., 1.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 300: cat - cat || Loss: 0.24440284073352814\n",
      "tensor([0., 1.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 301: dog - cat || Loss: 0.2558002173900604\n",
      "tensor([1., 0.]) tensor([0.4942, 0.5058], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 302: dog - cat || Loss: 0.25582408905029297\n",
      "tensor([1., 0.]) tensor([0.4942, 0.5058], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 303: cat - cat || Loss: 0.24432146549224854\n",
      "tensor([0., 1.]) tensor([0.4943, 0.5057], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 304: cat - cat || Loss: 0.24429485201835632\n",
      "tensor([0., 1.]) tensor([0.4943, 0.5057], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 305: cat - cat || Loss: 0.24417361617088318\n",
      "tensor([0., 1.]) tensor([0.4941, 0.5059], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 306: dog - cat || Loss: 0.2561061680316925\n",
      "tensor([1., 0.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 307: dog - cat || Loss: 0.2561943233013153\n",
      "tensor([1., 0.]) tensor([0.4938, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 308: dog - cat || Loss: 0.25617173314094543\n",
      "tensor([1., 0.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 309: cat - cat || Loss: 0.24402283132076263\n",
      "tensor([0., 1.]) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 310: cat - cat || Loss: 0.2440331131219864\n",
      "tensor([0., 1.]) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 311: cat - cat || Loss: 0.24394525587558746\n",
      "tensor([0., 1.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 312: dog - cat || Loss: 0.25630930066108704\n",
      "tensor([1., 0.]) tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 313: cat - cat || Loss: 0.24371030926704407\n",
      "tensor([0., 1.]) tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 314: cat - cat || Loss: 0.2435602843761444\n",
      "tensor([0., 1.]) tensor([0.4935, 0.5065], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 315: cat - cat || Loss: 0.24332842230796814\n",
      "tensor([0., 1.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 316: dog - cat || Loss: 0.25707563757896423\n",
      "tensor([1., 0.]) tensor([0.4930, 0.5070], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 317: dog - cat || Loss: 0.2572559714317322\n",
      "tensor([1., 0.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 318: dog - cat || Loss: 0.25731584429740906\n",
      "tensor([1., 0.]) tensor([0.4927, 0.5073], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 319: cat - cat || Loss: 0.24283674359321594\n",
      "tensor([0., 1.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 320: cat - cat || Loss: 0.24278242886066437\n",
      "tensor([0., 1.]) tensor([0.4927, 0.5073], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 321: cat - cat || Loss: 0.24263709783554077\n",
      "tensor([0., 1.]) tensor([0.4926, 0.5074], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 322: cat - cat || Loss: 0.24240973591804504\n",
      "tensor([0., 1.]) tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 323: cat - cat || Loss: 0.24210870265960693\n",
      "tensor([0., 1.]) tensor([0.4920, 0.5080], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 324: cat - cat || Loss: 0.24174174666404724\n",
      "tensor([0., 1.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 325: cat - cat || Loss: 0.24131560325622559\n",
      "tensor([0., 1.]) tensor([0.4912, 0.5088], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 326: cat - cat || Loss: 0.24083644151687622\n",
      "tensor([0., 1.]) tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 327: dog - cat || Loss: 0.25988173484802246\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 328: dog - cat || Loss: 0.26027098298072815\n",
      "tensor([1., 0.]) tensor([0.4898, 0.5102], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 329: dog - cat || Loss: 0.260517954826355\n",
      "tensor([1., 0.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 330: dog - cat || Loss: 0.26063674688339233\n",
      "tensor([1., 0.]) tensor([0.4895, 0.5105], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 331: cat - cat || Loss: 0.2395819127559662\n",
      "tensor([0., 1.]) tensor([0.4895, 0.5105], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 332: dog - cat || Loss: 0.26074206829071045\n",
      "tensor([1., 0.]) tensor([0.4894, 0.5106], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 333: dog - cat || Loss: 0.2607303857803345\n",
      "tensor([1., 0.]) tensor([0.4894, 0.5106], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 334: cat - cat || Loss: 0.23960457742214203\n",
      "tensor([0., 1.]) tensor([0.4895, 0.5105], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 335: dog - cat || Loss: 0.26061275601387024\n",
      "tensor([1., 0.]) tensor([0.4895, 0.5105], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 336: dog - cat || Loss: 0.2605060338973999\n",
      "tensor([1., 0.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 337: dog - cat || Loss: 0.2603064179420471\n",
      "tensor([1., 0.]) tensor([0.4898, 0.5102], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 338: cat - cat || Loss: 0.24017371237277985\n",
      "tensor([0., 1.]) tensor([0.4901, 0.5099], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 339: cat - cat || Loss: 0.240323007106781\n",
      "tensor([0., 1.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 340: cat - cat || Loss: 0.2403617948293686\n",
      "tensor([0., 1.]) tensor([0.4903, 0.5097], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 341: dog - cat || Loss: 0.2598907947540283\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 342: dog - cat || Loss: 0.25984418392181396\n",
      "tensor([1., 0.]) tensor([0.4903, 0.5097], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 343: cat - cat || Loss: 0.24048559367656708\n",
      "tensor([0., 1.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 344: cat - cat || Loss: 0.24051566421985626\n",
      "tensor([0., 1.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 345: cat - cat || Loss: 0.24044716358184814\n",
      "tensor([0., 1.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 346: dog - cat || Loss: 0.2599025368690491\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 347: dog - cat || Loss: 0.25994637608528137\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 348: dog - cat || Loss: 0.259882390499115\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 349: cat - cat || Loss: 0.2404639720916748\n",
      "tensor([0., 1.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 350: cat - cat || Loss: 0.2405075579881668\n",
      "tensor([0., 1.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 351: dog - cat || Loss: 0.25973471999168396\n",
      "tensor([1., 0.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 352: cat - cat || Loss: 0.24049988389015198\n",
      "tensor([0., 1.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 353: cat - cat || Loss: 0.2404480129480362\n",
      "tensor([0., 1.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 354: dog - cat || Loss: 0.2598859369754791\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 355: dog - cat || Loss: 0.259915828704834\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 356: cat - cat || Loss: 0.24035073816776276\n",
      "tensor([0., 1.]) tensor([0.4903, 0.5097], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 357: dog - cat || Loss: 0.2598697245121002\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 358: dog - cat || Loss: 0.259793758392334\n",
      "tensor([1., 0.]) tensor([0.4903, 0.5097], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 359: dog - cat || Loss: 0.25962206721305847\n",
      "tensor([1., 0.]) tensor([0.4905, 0.5095], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 360: cat - cat || Loss: 0.24080780148506165\n",
      "tensor([0., 1.]) tensor([0.4907, 0.5093], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 361: cat - cat || Loss: 0.24093542993068695\n",
      "tensor([0., 1.]) tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 362: dog - cat || Loss: 0.2592122554779053\n",
      "tensor([1., 0.]) tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 363: dog - cat || Loss: 0.2590913772583008\n",
      "tensor([1., 0.]) tensor([0.4910, 0.5090], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 364: cat - cat || Loss: 0.2412753701210022\n",
      "tensor([0., 1.]) tensor([0.4912, 0.5088], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 365: cat - cat || Loss: 0.24136337637901306\n",
      "tensor([0., 1.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 366: dog - cat || Loss: 0.25880587100982666\n",
      "tensor([1., 0.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 367: dog - cat || Loss: 0.25871846079826355\n",
      "tensor([1., 0.]) tensor([0.4914, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 368: dog - cat || Loss: 0.2585372030735016\n",
      "tensor([1., 0.]) tensor([0.4915, 0.5085], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 0 - 369: dog - cat || Loss: 0.2582710385322571\n",
      "tensor([1., 0.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "=====epoch:1=====\n",
      "Epoch 1 - 0: dog - cat || Loss: 0.2579290568828583\n",
      "tensor([1., 0.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 1: dog - cat || Loss: 0.2575189769268036\n",
      "tensor([1., 0.]) tensor([0.4925, 0.5075], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 2: cat - cat || Loss: 0.24305015802383423\n",
      "tensor([0., 1.]) tensor([0.4930, 0.5070], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 3: cat - cat || Loss: 0.24336569011211395\n",
      "tensor([0., 1.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 4: cat - cat || Loss: 0.243553027510643\n",
      "tensor([0., 1.]) tensor([0.4935, 0.5065], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 5: cat - cat || Loss: 0.24362482130527496\n",
      "tensor([0., 1.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 6: cat - cat || Loss: 0.24359245598316193\n",
      "tensor([0., 1.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 7: dog - cat || Loss: 0.25662004947662354\n",
      "tensor([1., 0.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 8: cat - cat || Loss: 0.2434525489807129\n",
      "tensor([0., 1.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 9: dog - cat || Loss: 0.25674664974212646\n",
      "tensor([1., 0.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 10: dog - cat || Loss: 0.2567456066608429\n",
      "tensor([1., 0.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 11: cat - cat || Loss: 0.24344459176063538\n",
      "tensor([0., 1.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 12: cat - cat || Loss: 0.24343809485435486\n",
      "tensor([0., 1.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 13: dog - cat || Loss: 0.25675466656684875\n",
      "tensor([1., 0.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 14: dog - cat || Loss: 0.25674745440483093\n",
      "tensor([1., 0.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 15: cat - cat || Loss: 0.2434481382369995\n",
      "tensor([0., 1.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 16: cat - cat || Loss: 0.24344655871391296\n",
      "tensor([0., 1.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 17: dog - cat || Loss: 0.2567415237426758\n",
      "tensor([1., 0.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 18: dog - cat || Loss: 0.25673022866249084\n",
      "tensor([1., 0.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 19: cat - cat || Loss: 0.24346841871738434\n",
      "tensor([0., 1.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 20: dog - cat || Loss: 0.2566164433956146\n",
      "tensor([1., 0.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 21: dog - cat || Loss: 0.25651299953460693\n",
      "tensor([1., 0.]) tensor([0.4935, 0.5065], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 22: cat - cat || Loss: 0.24376100301742554\n",
      "tensor([0., 1.]) tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 23: dog - cat || Loss: 0.25624173879623413\n",
      "tensor([1., 0.]) tensor([0.4938, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 24: cat - cat || Loss: 0.24400143325328827\n",
      "tensor([0., 1.]) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 25: dog - cat || Loss: 0.25601744651794434\n",
      "tensor([1., 0.]) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 26: dog - cat || Loss: 0.2558671236038208\n",
      "tensor([1., 0.]) tensor([0.4942, 0.5058], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 27: cat - cat || Loss: 0.24443261325359344\n",
      "tensor([0., 1.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 28: cat - cat || Loss: 0.24454385042190552\n",
      "tensor([0., 1.]) tensor([0.4945, 0.5055], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 29: cat - cat || Loss: 0.2445467859506607\n",
      "tensor([0., 1.]) tensor([0.4945, 0.5055], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 30: dog - cat || Loss: 0.25560998916625977\n",
      "tensor([1., 0.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 31: cat - cat || Loss: 0.24446642398834229\n",
      "tensor([0., 1.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 32: cat - cat || Loss: 0.2443820983171463\n",
      "tensor([0., 1.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 33: dog - cat || Loss: 0.2558589279651642\n",
      "tensor([1., 0.]) tensor([0.4942, 0.5058], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 34: cat - cat || Loss: 0.24415263533592224\n",
      "tensor([0., 1.]) tensor([0.4941, 0.5059], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 35: dog - cat || Loss: 0.25606802105903625\n",
      "tensor([1., 0.]) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 36: dog - cat || Loss: 0.2561022937297821\n",
      "tensor([1., 0.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 37: cat - cat || Loss: 0.24404045939445496\n",
      "tensor([0., 1.]) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 38: cat - cat || Loss: 0.24400576949119568\n",
      "tensor([0., 1.]) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 39: dog - cat || Loss: 0.256198525428772\n",
      "tensor([1., 0.]) tensor([0.4938, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 40: cat - cat || Loss: 0.2438613772392273\n",
      "tensor([0., 1.]) tensor([0.4938, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 41: cat - cat || Loss: 0.2437499612569809\n",
      "tensor([0., 1.]) tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 42: cat - cat || Loss: 0.24355272948741913\n",
      "tensor([0., 1.]) tensor([0.4935, 0.5065], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 43: cat - cat || Loss: 0.24327850341796875\n",
      "tensor([0., 1.]) tensor([0.4932, 0.5068], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 44: cat - cat || Loss: 0.2429352104663849\n",
      "tensor([0., 1.]) tensor([0.4929, 0.5071], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 45: dog - cat || Loss: 0.2575834095478058\n",
      "tensor([1., 0.]) tensor([0.4925, 0.5075], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 46: dog - cat || Loss: 0.2578567862510681\n",
      "tensor([1., 0.]) tensor([0.4922, 0.5078], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 47: cat - cat || Loss: 0.24212555587291718\n",
      "tensor([0., 1.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 48: dog - cat || Loss: 0.25822925567626953\n",
      "tensor([1., 0.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 49: cat - cat || Loss: 0.24180422723293304\n",
      "tensor([0., 1.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 50: cat - cat || Loss: 0.24161812663078308\n",
      "tensor([0., 1.]) tensor([0.4915, 0.5085], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 51: dog - cat || Loss: 0.2587973475456238\n",
      "tensor([1., 0.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 52: dog - cat || Loss: 0.2589399516582489\n",
      "tensor([1., 0.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 53: dog - cat || Loss: 0.2589651644229889\n",
      "tensor([1., 0.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 54: dog - cat || Loss: 0.2588850259780884\n",
      "tensor([1., 0.]) tensor([0.4912, 0.5088], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 55: cat - cat || Loss: 0.24143929779529572\n",
      "tensor([0., 1.]) tensor([0.4914, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 56: dog - cat || Loss: 0.25865164399147034\n",
      "tensor([1., 0.]) tensor([0.4914, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 57: cat - cat || Loss: 0.24164552986621857\n",
      "tensor([0., 1.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 58: dog - cat || Loss: 0.25845620036125183\n",
      "tensor([1., 0.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 59: dog - cat || Loss: 0.25831714272499084\n",
      "tensor([1., 0.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 60: cat - cat || Loss: 0.24203942716121674\n",
      "tensor([0., 1.]) tensor([0.4920, 0.5080], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 61: dog - cat || Loss: 0.25798386335372925\n",
      "tensor([1., 0.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 62: dog - cat || Loss: 0.2577863335609436\n",
      "tensor([1., 0.]) tensor([0.4923, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 63: dog - cat || Loss: 0.2575061023235321\n",
      "tensor([1., 0.]) tensor([0.4925, 0.5075], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 64: dog - cat || Loss: 0.2571517825126648\n",
      "tensor([1., 0.]) tensor([0.4929, 0.5071], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 65: cat - cat || Loss: 0.2433585822582245\n",
      "tensor([0., 1.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 66: dog - cat || Loss: 0.25645169615745544\n",
      "tensor([1., 0.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 67: cat - cat || Loss: 0.24397484958171844\n",
      "tensor([0., 1.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 68: dog - cat || Loss: 0.2558804452419281\n",
      "tensor([1., 0.]) tensor([0.4942, 0.5058], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 69: dog - cat || Loss: 0.25558242201805115\n",
      "tensor([1., 0.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 70: cat - cat || Loss: 0.24484100937843323\n",
      "tensor([0., 1.]) tensor([0.4948, 0.5052], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 71: dog - cat || Loss: 0.2549797296524048\n",
      "tensor([1., 0.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 72: cat - cat || Loss: 0.24537447094917297\n",
      "tensor([0., 1.]) tensor([0.4954, 0.5046], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 73: cat - cat || Loss: 0.24555158615112305\n",
      "tensor([0., 1.]) tensor([0.4955, 0.5045], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 74: cat - cat || Loss: 0.2456134557723999\n",
      "tensor([0., 1.]) tensor([0.4956, 0.5044], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 75: cat - cat || Loss: 0.24557141959667206\n",
      "tensor([0., 1.]) tensor([0.4956, 0.5044], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 76: dog - cat || Loss: 0.25460612773895264\n",
      "tensor([1., 0.]) tensor([0.4954, 0.5046], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 77: cat - cat || Loss: 0.24541336297988892\n",
      "tensor([0., 1.]) tensor([0.4954, 0.5046], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 78: cat - cat || Loss: 0.2452954798936844\n",
      "tensor([0., 1.]) tensor([0.4953, 0.5047], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 79: cat - cat || Loss: 0.24509195983409882\n",
      "tensor([0., 1.]) tensor([0.4951, 0.5049], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 80: dog - cat || Loss: 0.2552430331707001\n",
      "tensor([1., 0.]) tensor([0.4948, 0.5052], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 81: cat - cat || Loss: 0.2446584701538086\n",
      "tensor([0., 1.]) tensor([0.4946, 0.5054], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 82: dog - cat || Loss: 0.25563931465148926\n",
      "tensor([1., 0.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 83: dog - cat || Loss: 0.25575384497642517\n",
      "tensor([1., 0.]) tensor([0.4943, 0.5057], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 84: dog - cat || Loss: 0.25575515627861023\n",
      "tensor([1., 0.]) tensor([0.4943, 0.5057], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 85: cat - cat || Loss: 0.24440839886665344\n",
      "tensor([0., 1.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 86: dog - cat || Loss: 0.25566378235816956\n",
      "tensor([1., 0.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 87: dog - cat || Loss: 0.2555702328681946\n",
      "tensor([1., 0.]) tensor([0.4945, 0.5055], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 88: dog - cat || Loss: 0.25538432598114014\n",
      "tensor([1., 0.]) tensor([0.4946, 0.5054], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 89: cat - cat || Loss: 0.24493607878684998\n",
      "tensor([0., 1.]) tensor([0.4949, 0.5051], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 90: cat - cat || Loss: 0.24507549405097961\n",
      "tensor([0., 1.]) tensor([0.4951, 0.5049], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 91: dog - cat || Loss: 0.25494474172592163\n",
      "tensor([1., 0.]) tensor([0.4951, 0.5049], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 92: cat - cat || Loss: 0.24522842466831207\n",
      "tensor([0., 1.]) tensor([0.4952, 0.5048], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 93: cat - cat || Loss: 0.24524307250976562\n",
      "tensor([0., 1.]) tensor([0.4952, 0.5048], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 94: cat - cat || Loss: 0.245158851146698\n",
      "tensor([0., 1.]) tensor([0.4951, 0.5049], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 95: dog - cat || Loss: 0.2550651729106903\n",
      "tensor([1., 0.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 96: dog - cat || Loss: 0.25512292981147766\n",
      "tensor([1., 0.]) tensor([0.4949, 0.5051], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 97: dog - cat || Loss: 0.2550733685493469\n",
      "tensor([1., 0.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 98: cat - cat || Loss: 0.24512070417404175\n",
      "tensor([0., 1.]) tensor([0.4951, 0.5049], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 99: dog - cat || Loss: 0.2548954486846924\n",
      "tensor([1., 0.]) tensor([0.4951, 0.5049], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 100: cat - cat || Loss: 0.2452796995639801\n",
      "tensor([0., 1.]) tensor([0.4953, 0.5047], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 101: cat - cat || Loss: 0.24529702961444855\n",
      "tensor([0., 1.]) tensor([0.4953, 0.5047], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 102: cat - cat || Loss: 0.24521511793136597\n",
      "tensor([0., 1.]) tensor([0.4952, 0.5048], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 103: dog - cat || Loss: 0.2550056278705597\n",
      "tensor([1., 0.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 104: dog - cat || Loss: 0.25506144762039185\n",
      "tensor([1., 0.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 105: cat - cat || Loss: 0.24503952264785767\n",
      "tensor([0., 1.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 106: cat - cat || Loss: 0.24498730897903442\n",
      "tensor([0., 1.]) tensor([0.4950, 0.5050], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 107: dog - cat || Loss: 0.25521084666252136\n",
      "tensor([1., 0.]) tensor([0.4948, 0.5052], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 108: cat - cat || Loss: 0.24481239914894104\n",
      "tensor([0., 1.]) tensor([0.4948, 0.5052], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 109: cat - cat || Loss: 0.24468757212162018\n",
      "tensor([0., 1.]) tensor([0.4947, 0.5053], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 110: dog - cat || Loss: 0.25558364391326904\n",
      "tensor([1., 0.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 111: cat - cat || Loss: 0.2443888634443283\n",
      "tensor([0., 1.]) tensor([0.4944, 0.5056], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 112: dog - cat || Loss: 0.25585636496543884\n",
      "tensor([1., 0.]) tensor([0.4942, 0.5058], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 113: cat - cat || Loss: 0.24415120482444763\n",
      "tensor([0., 1.]) tensor([0.4941, 0.5059], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 114: cat - cat || Loss: 0.24399998784065247\n",
      "tensor([0., 1.]) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 115: cat - cat || Loss: 0.24376700818538666\n",
      "tensor([0., 1.]) tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 116: dog - cat || Loss: 0.25662606954574585\n",
      "tensor([1., 0.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 117: cat - cat || Loss: 0.2432841807603836\n",
      "tensor([0., 1.]) tensor([0.4932, 0.5068], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 118: cat - cat || Loss: 0.2430288940668106\n",
      "tensor([0., 1.]) tensor([0.4930, 0.5070], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 119: dog - cat || Loss: 0.257405549287796\n",
      "tensor([1., 0.]) tensor([0.4926, 0.5074], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 120: cat - cat || Loss: 0.24250845611095428\n",
      "tensor([0., 1.]) tensor([0.4925, 0.5075], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 121: cat - cat || Loss: 0.242237389087677\n",
      "tensor([0., 1.]) tensor([0.4922, 0.5078], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 122: cat - cat || Loss: 0.24189728498458862\n",
      "tensor([0., 1.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 123: cat - cat || Loss: 0.24149537086486816\n",
      "tensor([0., 1.]) tensor([0.4914, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 124: cat - cat || Loss: 0.24103792011737823\n",
      "tensor([0., 1.]) tensor([0.4910, 0.5090], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 125: cat - cat || Loss: 0.24053090810775757\n",
      "tensor([0., 1.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 126: cat - cat || Loss: 0.2399793416261673\n",
      "tensor([0., 1.]) tensor([0.4899, 0.5101], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 127: dog - cat || Loss: 0.2608417272567749\n",
      "tensor([1., 0.]) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 128: cat - cat || Loss: 0.23895643651485443\n",
      "tensor([0., 1.]) tensor([0.4888, 0.5112], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 129: cat - cat || Loss: 0.23847313225269318\n",
      "tensor([0., 1.]) tensor([0.4883, 0.5117], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 130: dog - cat || Loss: 0.2623539865016937\n",
      "tensor([1., 0.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 131: dog - cat || Loss: 0.2627497613430023\n",
      "tensor([1., 0.]) tensor([0.4874, 0.5126], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 132: cat - cat || Loss: 0.23732790350914001\n",
      "tensor([0., 1.]) tensor([0.4872, 0.5128], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 133: dog - cat || Loss: 0.2633278965950012\n",
      "tensor([1., 0.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 134: dog - cat || Loss: 0.263516902923584\n",
      "tensor([1., 0.]) tensor([0.4867, 0.5133], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 135: cat - cat || Loss: 0.23677700757980347\n",
      "tensor([0., 1.]) tensor([0.4866, 0.5134], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 136: dog - cat || Loss: 0.2637404203414917\n",
      "tensor([1., 0.]) tensor([0.4864, 0.5136], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 137: dog - cat || Loss: 0.26377803087234497\n",
      "tensor([1., 0.]) tensor([0.4864, 0.5136], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 138: dog - cat || Loss: 0.2637069821357727\n",
      "tensor([1., 0.]) tensor([0.4865, 0.5135], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 139: dog - cat || Loss: 0.2635382413864136\n",
      "tensor([1., 0.]) tensor([0.4866, 0.5134], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 140: cat - cat || Loss: 0.23706194758415222\n",
      "tensor([0., 1.]) tensor([0.4869, 0.5131], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 141: dog - cat || Loss: 0.2631503939628601\n",
      "tensor([1., 0.]) tensor([0.4870, 0.5130], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 142: dog - cat || Loss: 0.26292750239372253\n",
      "tensor([1., 0.]) tensor([0.4872, 0.5128], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 143: cat - cat || Loss: 0.23768839240074158\n",
      "tensor([0., 1.]) tensor([0.4875, 0.5125], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 144: dog - cat || Loss: 0.26244744658470154\n",
      "tensor([1., 0.]) tensor([0.4877, 0.5123], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 145: cat - cat || Loss: 0.2381042242050171\n",
      "tensor([0., 1.]) tensor([0.4880, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 146: cat - cat || Loss: 0.2382340431213379\n",
      "tensor([0., 1.]) tensor([0.4881, 0.5119], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 147: cat - cat || Loss: 0.23825623095035553\n",
      "tensor([0., 1.]) tensor([0.4881, 0.5119], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 148: dog - cat || Loss: 0.26210466027259827\n",
      "tensor([1., 0.]) tensor([0.4880, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 149: cat - cat || Loss: 0.23821347951889038\n",
      "tensor([0., 1.]) tensor([0.4881, 0.5119], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 150: cat - cat || Loss: 0.23814769089221954\n",
      "tensor([0., 1.]) tensor([0.4880, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 151: dog - cat || Loss: 0.26230159401893616\n",
      "tensor([1., 0.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 152: cat - cat || Loss: 0.23795467615127563\n",
      "tensor([0., 1.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 153: cat - cat || Loss: 0.23782490193843842\n",
      "tensor([0., 1.]) tensor([0.4877, 0.5123], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 154: dog - cat || Loss: 0.26270100474357605\n",
      "tensor([1., 0.]) tensor([0.4875, 0.5125], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 155: cat - cat || Loss: 0.23752282559871674\n",
      "tensor([0., 1.]) tensor([0.4874, 0.5126], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 156: dog - cat || Loss: 0.26298171281814575\n",
      "tensor([1., 0.]) tensor([0.4872, 0.5128], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 157: cat - cat || Loss: 0.23728768527507782\n",
      "tensor([0., 1.]) tensor([0.4871, 0.5129], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 158: cat - cat || Loss: 0.23714010417461395\n",
      "tensor([0., 1.]) tensor([0.4870, 0.5130], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 159: dog - cat || Loss: 0.26343849301338196\n",
      "tensor([1., 0.]) tensor([0.4867, 0.5133], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 160: dog - cat || Loss: 0.26354917883872986\n",
      "tensor([1., 0.]) tensor([0.4866, 0.5134], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 161: cat - cat || Loss: 0.23681329190731049\n",
      "tensor([0., 1.]) tensor([0.4866, 0.5134], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 162: cat - cat || Loss: 0.2367236167192459\n",
      "tensor([0., 1.]) tensor([0.4865, 0.5135], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 163: cat - cat || Loss: 0.23654882609844208\n",
      "tensor([0., 1.]) tensor([0.4864, 0.5136], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 164: cat - cat || Loss: 0.2362976223230362\n",
      "tensor([0., 1.]) tensor([0.4861, 0.5139], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 165: dog - cat || Loss: 0.2644268870353699\n",
      "tensor([1., 0.]) tensor([0.4858, 0.5142], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 166: dog - cat || Loss: 0.2646263837814331\n",
      "tensor([1., 0.]) tensor([0.4856, 0.5144], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 167: dog - cat || Loss: 0.26470082998275757\n",
      "tensor([1., 0.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 168: cat - cat || Loss: 0.23575513064861298\n",
      "tensor([0., 1.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 169: cat - cat || Loss: 0.23569387197494507\n",
      "tensor([0., 1.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 170: cat - cat || Loss: 0.2355450987815857\n",
      "tensor([0., 1.]) tensor([0.4853, 0.5147], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 171: cat - cat || Loss: 0.23531769216060638\n",
      "tensor([0., 1.]) tensor([0.4851, 0.5149], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 172: dog - cat || Loss: 0.26544320583343506\n",
      "tensor([1., 0.]) tensor([0.4848, 0.5152], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 173: dog - cat || Loss: 0.26562270522117615\n",
      "tensor([1., 0.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 174: dog - cat || Loss: 0.2656787633895874\n",
      "tensor([1., 0.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 175: cat - cat || Loss: 0.23484984040260315\n",
      "tensor([0., 1.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 176: cat - cat || Loss: 0.23480325937271118\n",
      "tensor([0., 1.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 177: dog - cat || Loss: 0.26581722497940063\n",
      "tensor([1., 0.]) tensor([0.4844, 0.5156], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 178: cat - cat || Loss: 0.23464539647102356\n",
      "tensor([0., 1.]) tensor([0.4844, 0.5156], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 179: cat - cat || Loss: 0.23453198373317719\n",
      "tensor([0., 1.]) tensor([0.4843, 0.5157], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 180: cat - cat || Loss: 0.23433665931224823\n",
      "tensor([0., 1.]) tensor([0.4841, 0.5159], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 181: dog - cat || Loss: 0.26645663380622864\n",
      "tensor([1., 0.]) tensor([0.4838, 0.5162], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 182: cat - cat || Loss: 0.23392529785633087\n",
      "tensor([0., 1.]) tensor([0.4837, 0.5163], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 183: cat - cat || Loss: 0.2337041050195694\n",
      "tensor([0., 1.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 184: dog - cat || Loss: 0.26715704798698425\n",
      "tensor([1., 0.]) tensor([0.4831, 0.5169], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 185: cat - cat || Loss: 0.23324896395206451\n",
      "tensor([0., 1.]) tensor([0.4830, 0.5170], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 186: dog - cat || Loss: 0.26758843660354614\n",
      "tensor([1., 0.]) tensor([0.4827, 0.5173], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 187: dog - cat || Loss: 0.2677130401134491\n",
      "tensor([1., 0.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 188: cat - cat || Loss: 0.2328878492116928\n",
      "tensor([0., 1.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 189: dog - cat || Loss: 0.2678232789039612\n",
      "tensor([1., 0.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 190: cat - cat || Loss: 0.23280200362205505\n",
      "tensor([0., 1.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 191: dog - cat || Loss: 0.267898827791214\n",
      "tensor([1., 0.]) tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 192: cat - cat || Loss: 0.23274533450603485\n",
      "tensor([0., 1.]) tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 193: cat - cat || Loss: 0.23267564177513123\n",
      "tensor([0., 1.]) tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 194: dog - cat || Loss: 0.26811298727989197\n",
      "tensor([1., 0.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 195: dog - cat || Loss: 0.2681562900543213\n",
      "tensor([1., 0.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 196: cat - cat || Loss: 0.2325429618358612\n",
      "tensor([0., 1.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 197: dog - cat || Loss: 0.2681272029876709\n",
      "tensor([1., 0.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 198: cat - cat || Loss: 0.2325742244720459\n",
      "tensor([0., 1.]) tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 199: cat - cat || Loss: 0.23254212737083435\n",
      "tensor([0., 1.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 200: cat - cat || Loss: 0.23242081701755524\n",
      "tensor([0., 1.]) tensor([0.4821, 0.5179], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 201: cat - cat || Loss: 0.23221927881240845\n",
      "tensor([0., 1.]) tensor([0.4819, 0.5181], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 202: dog - cat || Loss: 0.2687307298183441\n",
      "tensor([1., 0.]) tensor([0.4816, 0.5184], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 203: cat - cat || Loss: 0.23179903626441956\n",
      "tensor([0., 1.]) tensor([0.4815, 0.5185], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 204: dog - cat || Loss: 0.2691303491592407\n",
      "tensor([1., 0.]) tensor([0.4812, 0.5188], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 205: dog - cat || Loss: 0.2692408561706543\n",
      "tensor([1., 0.]) tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 206: cat - cat || Loss: 0.23147936165332794\n",
      "tensor([0., 1.]) tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 207: dog - cat || Loss: 0.26932576298713684\n",
      "tensor([1., 0.]) tensor([0.4810, 0.5190], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 208: cat - cat || Loss: 0.23141586780548096\n",
      "tensor([0., 1.]) tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 209: dog - cat || Loss: 0.269379585981369\n",
      "tensor([1., 0.]) tensor([0.4810, 0.5190], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 210: dog - cat || Loss: 0.2693425416946411\n",
      "tensor([1., 0.]) tensor([0.4810, 0.5190], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 211: cat - cat || Loss: 0.23150835931301117\n",
      "tensor([0., 1.]) tensor([0.4812, 0.5188], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 212: dog - cat || Loss: 0.2691750228404999\n",
      "tensor([1., 0.]) tensor([0.4812, 0.5188], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 213: dog - cat || Loss: 0.26904362440109253\n",
      "tensor([1., 0.]) tensor([0.4813, 0.5187], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 214: cat - cat || Loss: 0.23186425864696503\n",
      "tensor([0., 1.]) tensor([0.4815, 0.5185], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 215: dog - cat || Loss: 0.2687152028083801\n",
      "tensor([1., 0.]) tensor([0.4816, 0.5184], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 216: cat - cat || Loss: 0.23214593529701233\n",
      "tensor([0., 1.]) tensor([0.4818, 0.5182], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 217: cat - cat || Loss: 0.23222076892852783\n",
      "tensor([0., 1.]) tensor([0.4819, 0.5181], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 218: dog - cat || Loss: 0.26846182346343994\n",
      "tensor([1., 0.]) tensor([0.4819, 0.5181], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 219: dog - cat || Loss: 0.26837918162345886\n",
      "tensor([1., 0.]) tensor([0.4819, 0.5181], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 220: cat - cat || Loss: 0.2324410229921341\n",
      "tensor([0., 1.]) tensor([0.4821, 0.5179], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 221: dog - cat || Loss: 0.2681346833705902\n",
      "tensor([1., 0.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 222: cat - cat || Loss: 0.23265278339385986\n",
      "tensor([0., 1.]) tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 223: dog - cat || Loss: 0.2679228186607361\n",
      "tensor([1., 0.]) tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 224: cat - cat || Loss: 0.2328372597694397\n",
      "tensor([0., 1.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 225: cat - cat || Loss: 0.232870414853096\n",
      "tensor([0., 1.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 226: dog - cat || Loss: 0.26780468225479126\n",
      "tensor([1., 0.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 227: dog - cat || Loss: 0.2677587568759918\n",
      "tensor([1., 0.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 228: dog - cat || Loss: 0.26761114597320557\n",
      "tensor([1., 0.]) tensor([0.4827, 0.5173], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 229: dog - cat || Loss: 0.2673718333244324\n",
      "tensor([1., 0.]) tensor([0.4829, 0.5171], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 230: cat - cat || Loss: 0.23351195454597473\n",
      "tensor([0., 1.]) tensor([0.4832, 0.5168], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 231: cat - cat || Loss: 0.23368960618972778\n",
      "tensor([0., 1.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 232: dog - cat || Loss: 0.26678895950317383\n",
      "tensor([1., 0.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 233: cat - cat || Loss: 0.23391622304916382\n",
      "tensor([0., 1.]) tensor([0.4836, 0.5164], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 234: cat - cat || Loss: 0.2339669167995453\n",
      "tensor([0., 1.]) tensor([0.4837, 0.5163], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 235: cat - cat || Loss: 0.23391956090927124\n",
      "tensor([0., 1.]) tensor([0.4837, 0.5163], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 236: cat - cat || Loss: 0.23378406465053558\n",
      "tensor([0., 1.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 237: dog - cat || Loss: 0.26698923110961914\n",
      "tensor([1., 0.]) tensor([0.4833, 0.5167], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 238: dog - cat || Loss: 0.2670897841453552\n",
      "tensor([1., 0.]) tensor([0.4832, 0.5168], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 239: dog - cat || Loss: 0.2670742869377136\n",
      "tensor([1., 0.]) tensor([0.4832, 0.5168], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 240: dog - cat || Loss: 0.2669540345668793\n",
      "tensor([1., 0.]) tensor([0.4833, 0.5167], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 241: dog - cat || Loss: 0.2667398452758789\n",
      "tensor([1., 0.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 242: cat - cat || Loss: 0.23408235609531403\n",
      "tensor([0., 1.]) tensor([0.4838, 0.5162], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 243: cat - cat || Loss: 0.2342412769794464\n",
      "tensor([0., 1.]) tensor([0.4840, 0.5160], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 244: cat - cat || Loss: 0.23429112136363983\n",
      "tensor([0., 1.]) tensor([0.4840, 0.5160], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 245: cat - cat || Loss: 0.23424293100833893\n",
      "tensor([0., 1.]) tensor([0.4840, 0.5160], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 246: cat - cat || Loss: 0.23410657048225403\n",
      "tensor([0., 1.]) tensor([0.4838, 0.5162], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 247: cat - cat || Loss: 0.2338908463716507\n",
      "tensor([0., 1.]) tensor([0.4836, 0.5164], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 248: dog - cat || Loss: 0.26695215702056885\n",
      "tensor([1., 0.]) tensor([0.4833, 0.5167], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 249: dog - cat || Loss: 0.2671220600605011\n",
      "tensor([1., 0.]) tensor([0.4832, 0.5168], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 250: dog - cat || Loss: 0.26716893911361694\n",
      "tensor([1., 0.]) tensor([0.4831, 0.5169], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 251: cat - cat || Loss: 0.23346100747585297\n",
      "tensor([0., 1.]) tensor([0.4832, 0.5168], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 252: cat - cat || Loss: 0.23342208564281464\n",
      "tensor([0., 1.]) tensor([0.4831, 0.5169], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 253: dog - cat || Loss: 0.26728326082229614\n",
      "tensor([1., 0.]) tensor([0.4830, 0.5170], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 254: cat - cat || Loss: 0.23327858746051788\n",
      "tensor([0., 1.]) tensor([0.4830, 0.5170], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 255: dog - cat || Loss: 0.26741451025009155\n",
      "tensor([1., 0.]) tensor([0.4829, 0.5171], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 256: dog - cat || Loss: 0.2674112617969513\n",
      "tensor([1., 0.]) tensor([0.4829, 0.5171], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 257: dog - cat || Loss: 0.26730212569236755\n",
      "tensor([1., 0.]) tensor([0.4830, 0.5170], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 258: dog - cat || Loss: 0.267097532749176\n",
      "tensor([1., 0.]) tensor([0.4832, 0.5168], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 259: cat - cat || Loss: 0.2337392270565033\n",
      "tensor([0., 1.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 260: cat - cat || Loss: 0.2338906079530716\n",
      "tensor([0., 1.]) tensor([0.4836, 0.5164], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 261: cat - cat || Loss: 0.2339339405298233\n",
      "tensor([0., 1.]) tensor([0.4837, 0.5163], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 262: cat - cat || Loss: 0.23388001322746277\n",
      "tensor([0., 1.]) tensor([0.4836, 0.5164], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 263: dog - cat || Loss: 0.26680830121040344\n",
      "tensor([1., 0.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 264: cat - cat || Loss: 0.23371049761772156\n",
      "tensor([0., 1.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 265: dog - cat || Loss: 0.2669644057750702\n",
      "tensor([1., 0.]) tensor([0.4833, 0.5167], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 266: dog - cat || Loss: 0.26697203516960144\n",
      "tensor([1., 0.]) tensor([0.4833, 0.5167], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 267: dog - cat || Loss: 0.26687273383140564\n",
      "tensor([1., 0.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 268: cat - cat || Loss: 0.23386108875274658\n",
      "tensor([0., 1.]) tensor([0.4836, 0.5164], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 269: dog - cat || Loss: 0.2666008174419403\n",
      "tensor([1., 0.]) tensor([0.4837, 0.5163], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 270: cat - cat || Loss: 0.23409655690193176\n",
      "tensor([0., 1.]) tensor([0.4838, 0.5162], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 271: dog - cat || Loss: 0.26636794209480286\n",
      "tensor([1., 0.]) tensor([0.4839, 0.5161], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 272: cat - cat || Loss: 0.23429922759532928\n",
      "tensor([0., 1.]) tensor([0.4840, 0.5160], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 273: dog - cat || Loss: 0.2661668658256531\n",
      "tensor([1., 0.]) tensor([0.4841, 0.5159], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 274: dog - cat || Loss: 0.2660224735736847\n",
      "tensor([1., 0.]) tensor([0.4842, 0.5158], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 275: dog - cat || Loss: 0.26578688621520996\n",
      "tensor([1., 0.]) tensor([0.4845, 0.5155], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 276: cat - cat || Loss: 0.2349950671195984\n",
      "tensor([0., 1.]) tensor([0.4848, 0.5152], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 277: dog - cat || Loss: 0.26528283953666687\n",
      "tensor([1., 0.]) tensor([0.4849, 0.5151], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 278: dog - cat || Loss: 0.2650097906589508\n",
      "tensor([1., 0.]) tensor([0.4852, 0.5148], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 279: dog - cat || Loss: 0.2646588981151581\n",
      "tensor([1., 0.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 280: cat - cat || Loss: 0.23615610599517822\n",
      "tensor([0., 1.]) tensor([0.4860, 0.5140], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 281: cat - cat || Loss: 0.2364201545715332\n",
      "tensor([0., 1.]) tensor([0.4862, 0.5138], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 282: dog - cat || Loss: 0.26380717754364014\n",
      "tensor([1., 0.]) tensor([0.4864, 0.5136], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 283: dog - cat || Loss: 0.26356571912765503\n",
      "tensor([1., 0.]) tensor([0.4866, 0.5134], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 284: dog - cat || Loss: 0.2632439136505127\n",
      "tensor([1., 0.]) tensor([0.4869, 0.5131], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 285: dog - cat || Loss: 0.26284992694854736\n",
      "tensor([1., 0.]) tensor([0.4873, 0.5127], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 286: dog - cat || Loss: 0.2623911201953888\n",
      "tensor([1., 0.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 287: dog - cat || Loss: 0.2618744671344757\n",
      "tensor([1., 0.]) tensor([0.4883, 0.5117], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 288: cat - cat || Loss: 0.23894408345222473\n",
      "tensor([0., 1.]) tensor([0.4888, 0.5112], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 289: dog - cat || Loss: 0.26089397072792053\n",
      "tensor([1., 0.]) tensor([0.4892, 0.5108], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 290: dog - cat || Loss: 0.2604199945926666\n",
      "tensor([1., 0.]) tensor([0.4897, 0.5103], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 291: cat - cat || Loss: 0.24030150473117828\n",
      "tensor([0., 1.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 292: dog - cat || Loss: 0.25951337814331055\n",
      "tensor([1., 0.]) tensor([0.4906, 0.5094], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 293: cat - cat || Loss: 0.2410903424024582\n",
      "tensor([0., 1.]) tensor([0.4910, 0.5090], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 294: dog - cat || Loss: 0.2587730288505554\n",
      "tensor([1., 0.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 295: cat - cat || Loss: 0.24173691868782043\n",
      "tensor([0., 1.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 296: dog - cat || Loss: 0.25816744565963745\n",
      "tensor([1., 0.]) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 297: cat - cat || Loss: 0.2422674000263214\n",
      "tensor([0., 1.]) tensor([0.4922, 0.5078], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 298: cat - cat || Loss: 0.24244455993175507\n",
      "tensor([0., 1.]) tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 299: cat - cat || Loss: 0.24250778555870056\n",
      "tensor([0., 1.]) tensor([0.4925, 0.5075], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 300: cat - cat || Loss: 0.24246829748153687\n",
      "tensor([0., 1.]) tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 301: dog - cat || Loss: 0.25778278708457947\n",
      "tensor([1., 0.]) tensor([0.4923, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 302: dog - cat || Loss: 0.25780272483825684\n",
      "tensor([1., 0.]) tensor([0.4923, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 303: cat - cat || Loss: 0.24239906668663025\n",
      "tensor([0., 1.]) tensor([0.4923, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 304: cat - cat || Loss: 0.24237650632858276\n",
      "tensor([0., 1.]) tensor([0.4923, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 305: cat - cat || Loss: 0.2422599047422409\n",
      "tensor([0., 1.]) tensor([0.4922, 0.5078], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 306: dog - cat || Loss: 0.25806930661201477\n",
      "tensor([1., 0.]) tensor([0.4920, 0.5080], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 307: dog - cat || Loss: 0.25815361738204956\n",
      "tensor([1., 0.]) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 308: dog - cat || Loss: 0.25812700390815735\n",
      "tensor([1., 0.]) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 309: cat - cat || Loss: 0.24212545156478882\n",
      "tensor([0., 1.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 310: cat - cat || Loss: 0.242139533162117\n",
      "tensor([0., 1.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 311: cat - cat || Loss: 0.2420560121536255\n",
      "tensor([0., 1.]) tensor([0.4920, 0.5080], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 312: dog - cat || Loss: 0.2582489848136902\n",
      "tensor([1., 0.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 313: cat - cat || Loss: 0.2418300360441208\n",
      "tensor([0., 1.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 314: cat - cat || Loss: 0.2416847050189972\n",
      "tensor([0., 1.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 315: cat - cat || Loss: 0.2414579540491104\n",
      "tensor([0., 1.]) tensor([0.4914, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 316: dog - cat || Loss: 0.2590009868144989\n",
      "tensor([1., 0.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 317: dog - cat || Loss: 0.2591777741909027\n",
      "tensor([1., 0.]) tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 318: dog - cat || Loss: 0.25923383235931396\n",
      "tensor([1., 0.]) tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 319: cat - cat || Loss: 0.24098415672779083\n",
      "tensor([0., 1.]) tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 320: cat - cat || Loss: 0.24093389511108398\n",
      "tensor([0., 1.]) tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 321: cat - cat || Loss: 0.2407931238412857\n",
      "tensor([0., 1.]) tensor([0.4907, 0.5093], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 322: cat - cat || Loss: 0.24057084321975708\n",
      "tensor([0., 1.]) tensor([0.4905, 0.5095], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 323: cat - cat || Loss: 0.2402752935886383\n",
      "tensor([0., 1.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 324: cat - cat || Loss: 0.2399141788482666\n",
      "tensor([0., 1.]) tensor([0.4898, 0.5102], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 325: cat - cat || Loss: 0.2394941747188568\n",
      "tensor([0., 1.]) tensor([0.4894, 0.5106], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 326: cat - cat || Loss: 0.2390214204788208\n",
      "tensor([0., 1.]) tensor([0.4889, 0.5111], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 327: dog - cat || Loss: 0.26176920533180237\n",
      "tensor([1., 0.]) tensor([0.4884, 0.5116], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 328: dog - cat || Loss: 0.2621553838253021\n",
      "tensor([1., 0.]) tensor([0.4880, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 329: dog - cat || Loss: 0.26239892840385437\n",
      "tensor([1., 0.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 330: dog - cat || Loss: 0.26251405477523804\n",
      "tensor([1., 0.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 331: cat - cat || Loss: 0.23779231309890747\n",
      "tensor([0., 1.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 332: dog - cat || Loss: 0.26261186599731445\n",
      "tensor([1., 0.]) tensor([0.4875, 0.5125], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 333: dog - cat || Loss: 0.2625963091850281\n",
      "tensor([1., 0.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 334: cat - cat || Loss: 0.23782581090927124\n",
      "tensor([0., 1.]) tensor([0.4877, 0.5123], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 335: dog - cat || Loss: 0.2624708116054535\n",
      "tensor([1., 0.]) tensor([0.4877, 0.5123], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 336: dog - cat || Loss: 0.2623600959777832\n",
      "tensor([1., 0.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 337: dog - cat || Loss: 0.26215633749961853\n",
      "tensor([1., 0.]) tensor([0.4880, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 338: cat - cat || Loss: 0.23840633034706116\n",
      "tensor([0., 1.]) tensor([0.4883, 0.5117], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 339: cat - cat || Loss: 0.23855841159820557\n",
      "tensor([0., 1.]) tensor([0.4884, 0.5116], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 340: cat - cat || Loss: 0.23860056698322296\n",
      "tensor([0., 1.]) tensor([0.4885, 0.5115], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 341: dog - cat || Loss: 0.26172494888305664\n",
      "tensor([1., 0.]) tensor([0.4884, 0.5116], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 342: dog - cat || Loss: 0.2616744637489319\n",
      "tensor([1., 0.]) tensor([0.4885, 0.5115], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 343: cat - cat || Loss: 0.2387343943119049\n",
      "tensor([0., 1.]) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 344: cat - cat || Loss: 0.2387678325176239\n",
      "tensor([0., 1.]) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 345: cat - cat || Loss: 0.23870326578617096\n",
      "tensor([0., 1.]) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 346: dog - cat || Loss: 0.2617180347442627\n",
      "tensor([1., 0.]) tensor([0.4884, 0.5116], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 347: dog - cat || Loss: 0.2617582678794861\n",
      "tensor([1., 0.]) tensor([0.4884, 0.5116], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 348: dog - cat || Loss: 0.26169052720069885\n",
      "tensor([1., 0.]) tensor([0.4884, 0.5116], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 349: cat - cat || Loss: 0.23873405158519745\n",
      "tensor([0., 1.]) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 350: cat - cat || Loss: 0.2387809157371521\n",
      "tensor([0., 1.]) tensor([0.4887, 0.5113], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 351: dog - cat || Loss: 0.26153165102005005\n",
      "tensor([1., 0.]) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 352: cat - cat || Loss: 0.23878026008605957\n",
      "tensor([0., 1.]) tensor([0.4887, 0.5113], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 353: cat - cat || Loss: 0.23873215913772583\n",
      "tensor([0., 1.]) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 354: dog - cat || Loss: 0.26167216897010803\n",
      "tensor([1., 0.]) tensor([0.4885, 0.5115], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 355: dog - cat || Loss: 0.26169848442077637\n",
      "tensor([1., 0.]) tensor([0.4884, 0.5116], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 356: cat - cat || Loss: 0.2386457920074463\n",
      "tensor([0., 1.]) tensor([0.4885, 0.5115], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 357: dog - cat || Loss: 0.26164504885673523\n",
      "tensor([1., 0.]) tensor([0.4885, 0.5115], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 358: dog - cat || Loss: 0.2615653872489929\n",
      "tensor([1., 0.]) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 359: dog - cat || Loss: 0.2613898813724518\n",
      "tensor([1., 0.]) tensor([0.4887, 0.5113], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 360: cat - cat || Loss: 0.23911410570144653\n",
      "tensor([0., 1.]) tensor([0.4890, 0.5110], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 361: cat - cat || Loss: 0.2392444759607315\n",
      "tensor([0., 1.]) tensor([0.4891, 0.5109], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 362: dog - cat || Loss: 0.26096877455711365\n",
      "tensor([1., 0.]) tensor([0.4891, 0.5109], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 363: dog - cat || Loss: 0.26084405183792114\n",
      "tensor([1., 0.]) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 364: cat - cat || Loss: 0.23959282040596008\n",
      "tensor([0., 1.]) tensor([0.4895, 0.5105], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 365: cat - cat || Loss: 0.23968373239040375\n",
      "tensor([0., 1.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 366: dog - cat || Loss: 0.26054757833480835\n",
      "tensor([1., 0.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 367: dog - cat || Loss: 0.26045653223991394\n",
      "tensor([1., 0.]) tensor([0.4897, 0.5103], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 368: dog - cat || Loss: 0.26027148962020874\n",
      "tensor([1., 0.]) tensor([0.4898, 0.5102], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 1 - 369: dog - cat || Loss: 0.2600015103816986\n",
      "tensor([1., 0.]) tensor([0.4901, 0.5099], grad_fn=<SoftmaxBackward0>)\n",
      "=====epoch:2=====\n",
      "Epoch 2 - 0: dog - cat || Loss: 0.2596554458141327\n",
      "tensor([1., 0.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 1: dog - cat || Loss: 0.2592412531375885\n",
      "tensor([1., 0.]) tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 2: cat - cat || Loss: 0.24138526618480682\n",
      "tensor([0., 1.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 3: cat - cat || Loss: 0.24170251190662384\n",
      "tensor([0., 1.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 4: cat - cat || Loss: 0.24189221858978271\n",
      "tensor([0., 1.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 5: cat - cat || Loss: 0.24196696281433105\n",
      "tensor([0., 1.]) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 6: cat - cat || Loss: 0.24193809926509857\n",
      "tensor([0., 1.]) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 7: dog - cat || Loss: 0.2583200931549072\n",
      "tensor([1., 0.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 8: cat - cat || Loss: 0.24180559813976288\n",
      "tensor([0., 1.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 9: dog - cat || Loss: 0.2584400773048401\n",
      "tensor([1., 0.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 10: dog - cat || Loss: 0.25843554735183716\n",
      "tensor([1., 0.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 11: cat - cat || Loss: 0.24180757999420166\n",
      "tensor([0., 1.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 12: cat - cat || Loss: 0.24180440604686737\n",
      "tensor([0., 1.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 13: dog - cat || Loss: 0.2584344744682312\n",
      "tensor([1., 0.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 14: dog - cat || Loss: 0.2584238350391388\n",
      "tensor([1., 0.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 15: cat - cat || Loss: 0.24182425439357758\n",
      "tensor([0., 1.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 16: cat - cat || Loss: 0.24182593822479248\n",
      "tensor([0., 1.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 17: dog - cat || Loss: 0.25840774178504944\n",
      "tensor([1., 0.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 18: dog - cat || Loss: 0.2583930790424347\n",
      "tensor([1., 0.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 19: cat - cat || Loss: 0.24185745418071747\n",
      "tensor([0., 1.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 20: dog - cat || Loss: 0.2582724094390869\n",
      "tensor([1., 0.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 21: dog - cat || Loss: 0.2581653892993927\n",
      "tensor([1., 0.]) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 22: cat - cat || Loss: 0.24215833842754364\n",
      "tensor([0., 1.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 23: dog - cat || Loss: 0.257887065410614\n",
      "tensor([1., 0.]) tensor([0.4922, 0.5078], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 24: cat - cat || Loss: 0.242404043674469\n",
      "tensor([0., 1.]) tensor([0.4923, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 25: dog - cat || Loss: 0.2576557695865631\n",
      "tensor([1., 0.]) tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 26: dog - cat || Loss: 0.25750190019607544\n",
      "tensor([1., 0.]) tensor([0.4926, 0.5074], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 27: cat - cat || Loss: 0.24284277856349945\n",
      "tensor([0., 1.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 28: cat - cat || Loss: 0.24295668303966522\n",
      "tensor([0., 1.]) tensor([0.4929, 0.5071], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 29: cat - cat || Loss: 0.2429627925157547\n",
      "tensor([0., 1.]) tensor([0.4929, 0.5071], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 30: dog - cat || Loss: 0.257231205701828\n",
      "tensor([1., 0.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 31: cat - cat || Loss: 0.2428891956806183\n",
      "tensor([0., 1.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 32: cat - cat || Loss: 0.24280846118927002\n",
      "tensor([0., 1.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 33: dog - cat || Loss: 0.25747063755989075\n",
      "tensor([1., 0.]) tensor([0.4926, 0.5074], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 34: cat - cat || Loss: 0.24258644878864288\n",
      "tensor([0., 1.]) tensor([0.4925, 0.5075], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 35: dog - cat || Loss: 0.25767356157302856\n",
      "tensor([1., 0.]) tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 36: dog - cat || Loss: 0.2577046751976013\n",
      "tensor([1., 0.]) tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 37: cat - cat || Loss: 0.24248427152633667\n",
      "tensor([0., 1.]) tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 38: cat - cat || Loss: 0.24245287477970123\n",
      "tensor([0., 1.]) tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 39: dog - cat || Loss: 0.25779134035110474\n",
      "tensor([1., 0.]) tensor([0.4923, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 40: cat - cat || Loss: 0.24231544137001038\n",
      "tensor([0., 1.]) tensor([0.4923, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 41: cat - cat || Loss: 0.24220769107341766\n",
      "tensor([0., 1.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 42: cat - cat || Loss: 0.24201451241970062\n",
      "tensor([0., 1.]) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 43: cat - cat || Loss: 0.241744726896286\n",
      "tensor([0., 1.]) tensor([0.4917, 0.5083], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 44: cat - cat || Loss: 0.2414061725139618\n",
      "tensor([0., 1.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 45: dog - cat || Loss: 0.2591588497161865\n",
      "tensor([1., 0.]) tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 46: dog - cat || Loss: 0.2594294846057892\n",
      "tensor([1., 0.]) tensor([0.4907, 0.5093], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 47: cat - cat || Loss: 0.24060966074466705\n",
      "tensor([0., 1.]) tensor([0.4905, 0.5095], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 48: dog - cat || Loss: 0.2597959637641907\n",
      "tensor([1., 0.]) tensor([0.4903, 0.5097], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 49: cat - cat || Loss: 0.24029600620269775\n",
      "tensor([0., 1.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 50: cat - cat || Loss: 0.24011386930942535\n",
      "tensor([0., 1.]) tensor([0.4900, 0.5100], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 51: dog - cat || Loss: 0.26035529375076294\n",
      "tensor([1., 0.]) tensor([0.4897, 0.5103], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 52: dog - cat || Loss: 0.2604949176311493\n",
      "tensor([1., 0.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 53: dog - cat || Loss: 0.2605170011520386\n",
      "tensor([1., 0.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 54: dog - cat || Loss: 0.2604336142539978\n",
      "tensor([1., 0.]) tensor([0.4897, 0.5103], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 55: cat - cat || Loss: 0.23995105922222137\n",
      "tensor([0., 1.]) tensor([0.4898, 0.5102], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 56: dog - cat || Loss: 0.2601936459541321\n",
      "tensor([1., 0.]) tensor([0.4899, 0.5101], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 57: cat - cat || Loss: 0.24016235768795013\n",
      "tensor([0., 1.]) tensor([0.4901, 0.5099], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 58: dog - cat || Loss: 0.25999167561531067\n",
      "tensor([1., 0.]) tensor([0.4901, 0.5099], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 59: dog - cat || Loss: 0.25984933972358704\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 60: cat - cat || Loss: 0.24056334793567657\n",
      "tensor([0., 1.]) tensor([0.4905, 0.5095], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 61: dog - cat || Loss: 0.25950950384140015\n",
      "tensor([1., 0.]) tensor([0.4906, 0.5094], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 62: dog - cat || Loss: 0.2593086361885071\n",
      "tensor([1., 0.]) tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 63: dog - cat || Loss: 0.259024977684021\n",
      "tensor([1., 0.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 64: dog - cat || Loss: 0.25866714119911194\n",
      "tensor([1., 0.]) tensor([0.4914, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 65: cat - cat || Loss: 0.24189111590385437\n",
      "tensor([0., 1.]) tensor([0.4918, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 66: dog - cat || Loss: 0.2579600512981415\n",
      "tensor([1., 0.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 67: cat - cat || Loss: 0.24251039326190948\n",
      "tensor([0., 1.]) tensor([0.4925, 0.5075], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 68: dog - cat || Loss: 0.25738200545310974\n",
      "tensor([1., 0.]) tensor([0.4927, 0.5073], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 69: dog - cat || Loss: 0.25708064436912537\n",
      "tensor([1., 0.]) tensor([0.4930, 0.5070], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 70: cat - cat || Loss: 0.2433813214302063\n",
      "tensor([0., 1.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 71: dog - cat || Loss: 0.2564711272716522\n",
      "tensor([1., 0.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 72: cat - cat || Loss: 0.24391815066337585\n",
      "tensor([0., 1.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 73: cat - cat || Loss: 0.24409736692905426\n",
      "tensor([0., 1.]) tensor([0.4941, 0.5059], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 74: cat - cat || Loss: 0.24416188895702362\n",
      "tensor([0., 1.]) tensor([0.4941, 0.5059], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 75: cat - cat || Loss: 0.24412301182746887\n",
      "tensor([0., 1.]) tensor([0.4941, 0.5059], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 76: dog - cat || Loss: 0.2560819983482361\n",
      "tensor([1., 0.]) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 77: cat - cat || Loss: 0.2439715713262558\n",
      "tensor([0., 1.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 78: cat - cat || Loss: 0.24385714530944824\n",
      "tensor([0., 1.]) tensor([0.4938, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 79: cat - cat || Loss: 0.2436574250459671\n",
      "tensor([0., 1.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 80: dog - cat || Loss: 0.25670772790908813\n",
      "tensor([1., 0.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 81: cat - cat || Loss: 0.24323174357414246\n",
      "tensor([0., 1.]) tensor([0.4932, 0.5068], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 82: dog - cat || Loss: 0.257098525762558\n",
      "tensor([1., 0.]) tensor([0.4930, 0.5070], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 83: dog - cat || Loss: 0.25721028447151184\n",
      "tensor([1., 0.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 84: dog - cat || Loss: 0.2572086453437805\n",
      "tensor([1., 0.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 85: cat - cat || Loss: 0.24299432337284088\n",
      "tensor([0., 1.]) tensor([0.4929, 0.5071], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 86: dog - cat || Loss: 0.2571112811565399\n",
      "tensor([1., 0.]) tensor([0.4929, 0.5071], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 87: dog - cat || Loss: 0.2570146918296814\n",
      "tensor([1., 0.]) tensor([0.4930, 0.5070], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 88: dog - cat || Loss: 0.256825715303421\n",
      "tensor([1., 0.]) tensor([0.4932, 0.5068], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 89: cat - cat || Loss: 0.24353086948394775\n",
      "tensor([0., 1.]) tensor([0.4935, 0.5065], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 90: cat - cat || Loss: 0.2436724752187729\n",
      "tensor([0., 1.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 91: dog - cat || Loss: 0.2563769519329071\n",
      "tensor([1., 0.]) tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 92: cat - cat || Loss: 0.2438303530216217\n",
      "tensor([0., 1.]) tensor([0.4938, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 93: cat - cat || Loss: 0.24384774267673492\n",
      "tensor([0., 1.]) tensor([0.4938, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 94: cat - cat || Loss: 0.24376673996448517\n",
      "tensor([0., 1.]) tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 95: dog - cat || Loss: 0.2564859688282013\n",
      "tensor([1., 0.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 96: dog - cat || Loss: 0.25654086470603943\n",
      "tensor([1., 0.]) tensor([0.4935, 0.5065], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 97: dog - cat || Loss: 0.2564884126186371\n",
      "tensor([1., 0.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 98: cat - cat || Loss: 0.24373994767665863\n",
      "tensor([0., 1.]) tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 99: dog - cat || Loss: 0.25630462169647217\n",
      "tensor([1., 0.]) tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 100: cat - cat || Loss: 0.2439037710428238\n",
      "tensor([0., 1.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 101: cat - cat || Loss: 0.24392379820346832\n",
      "tensor([0., 1.]) tensor([0.4939, 0.5061], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 102: cat - cat || Loss: 0.24384503066539764\n",
      "tensor([0., 1.]) tensor([0.4938, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 103: dog - cat || Loss: 0.25640353560447693\n",
      "tensor([1., 0.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 104: dog - cat || Loss: 0.25645655393600464\n",
      "tensor([1., 0.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 105: cat - cat || Loss: 0.2436785250902176\n",
      "tensor([0., 1.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 106: cat - cat || Loss: 0.2436293065547943\n",
      "tensor([0., 1.]) tensor([0.4936, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 107: dog - cat || Loss: 0.256597638130188\n",
      "tensor([1., 0.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 108: cat - cat || Loss: 0.24346065521240234\n",
      "tensor([0., 1.]) tensor([0.4934, 0.5066], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 109: cat - cat || Loss: 0.24333913624286652\n",
      "tensor([0., 1.]) tensor([0.4933, 0.5067], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 110: dog - cat || Loss: 0.2569623589515686\n",
      "tensor([1., 0.]) tensor([0.4931, 0.5069], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 111: cat - cat || Loss: 0.24304720759391785\n",
      "tensor([0., 1.]) tensor([0.4930, 0.5070], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 112: dog - cat || Loss: 0.2572297751903534\n",
      "tensor([1., 0.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 113: cat - cat || Loss: 0.2428160309791565\n",
      "tensor([0., 1.]) tensor([0.4928, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 114: cat - cat || Loss: 0.24266815185546875\n",
      "tensor([0., 1.]) tensor([0.4926, 0.5074], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 115: cat - cat || Loss: 0.24243886768817902\n",
      "tensor([0., 1.]) tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 116: dog - cat || Loss: 0.2579892575740814\n",
      "tensor([1., 0.]) tensor([0.4921, 0.5079], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 117: cat - cat || Loss: 0.24196341633796692\n",
      "tensor([0., 1.]) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 118: cat - cat || Loss: 0.24171185493469238\n",
      "tensor([0., 1.]) tensor([0.4916, 0.5084], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 119: dog - cat || Loss: 0.25876137614250183\n",
      "tensor([1., 0.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 120: cat - cat || Loss: 0.24119894206523895\n",
      "tensor([0., 1.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 121: cat - cat || Loss: 0.2409316599369049\n",
      "tensor([0., 1.]) tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 122: cat - cat || Loss: 0.24059565365314484\n",
      "tensor([0., 1.]) tensor([0.4905, 0.5095], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 123: cat - cat || Loss: 0.2401980757713318\n",
      "tensor([0., 1.]) tensor([0.4901, 0.5099], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 124: cat - cat || Loss: 0.23974521458148956\n",
      "tensor([0., 1.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 125: cat - cat || Loss: 0.23924298584461212\n",
      "tensor([0., 1.]) tensor([0.4891, 0.5109], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 126: cat - cat || Loss: 0.2386963963508606\n",
      "tensor([0., 1.]) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 127: dog - cat || Loss: 0.26217910647392273\n",
      "tensor([1., 0.]) tensor([0.4880, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 128: cat - cat || Loss: 0.23768284916877747\n",
      "tensor([0., 1.]) tensor([0.4875, 0.5125], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 129: cat - cat || Loss: 0.23720404505729675\n",
      "tensor([0., 1.]) tensor([0.4870, 0.5130], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 130: dog - cat || Loss: 0.2636852264404297\n",
      "tensor([1., 0.]) tensor([0.4865, 0.5135], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 131: dog - cat || Loss: 0.2640787959098816\n",
      "tensor([1., 0.]) tensor([0.4861, 0.5139], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 132: cat - cat || Loss: 0.2360709011554718\n",
      "tensor([0., 1.]) tensor([0.4859, 0.5141], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 133: dog - cat || Loss: 0.264652281999588\n",
      "tensor([1., 0.]) tensor([0.4856, 0.5144], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 134: dog - cat || Loss: 0.264838844537735\n",
      "tensor([1., 0.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 135: cat - cat || Loss: 0.23552975058555603\n",
      "tensor([0., 1.]) tensor([0.4853, 0.5147], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 136: dog - cat || Loss: 0.2650572955608368\n",
      "tensor([1., 0.]) tensor([0.4852, 0.5148], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 137: dog - cat || Loss: 0.26509228348731995\n",
      "tensor([1., 0.]) tensor([0.4851, 0.5149], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 138: dog - cat || Loss: 0.2650184631347656\n",
      "tensor([1., 0.]) tensor([0.4852, 0.5148], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 139: dog - cat || Loss: 0.2648468315601349\n",
      "tensor([1., 0.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 140: cat - cat || Loss: 0.2358262538909912\n",
      "tensor([0., 1.]) tensor([0.4856, 0.5144], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 141: dog - cat || Loss: 0.26445311307907104\n",
      "tensor([1., 0.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 142: dog - cat || Loss: 0.2642272710800171\n",
      "tensor([1., 0.]) tensor([0.4860, 0.5140], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 143: cat - cat || Loss: 0.23645783960819244\n",
      "tensor([0., 1.]) tensor([0.4863, 0.5137], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 144: dog - cat || Loss: 0.2637414038181305\n",
      "tensor([1., 0.]) tensor([0.4864, 0.5136], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 145: cat - cat || Loss: 0.23687706887722015\n",
      "tensor([0., 1.]) tensor([0.4867, 0.5133], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 146: cat - cat || Loss: 0.23700888454914093\n",
      "tensor([0., 1.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 147: cat - cat || Loss: 0.23703348636627197\n",
      "tensor([0., 1.]) tensor([0.4869, 0.5131], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 148: dog - cat || Loss: 0.26338762044906616\n",
      "tensor([1., 0.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 149: cat - cat || Loss: 0.23699583113193512\n",
      "tensor([0., 1.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 150: cat - cat || Loss: 0.23693275451660156\n",
      "tensor([0., 1.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 151: dog - cat || Loss: 0.26357701420783997\n",
      "tensor([1., 0.]) tensor([0.4866, 0.5134], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 152: cat - cat || Loss: 0.23674537241458893\n",
      "tensor([0., 1.]) tensor([0.4866, 0.5134], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 153: cat - cat || Loss: 0.2366185486316681\n",
      "tensor([0., 1.]) tensor([0.4864, 0.5136], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 154: dog - cat || Loss: 0.26396915316581726\n",
      "tensor([1., 0.]) tensor([0.4862, 0.5138], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 155: cat - cat || Loss: 0.23632247745990753\n",
      "tensor([0., 1.]) tensor([0.4861, 0.5139], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 156: dog - cat || Loss: 0.26424506306648254\n",
      "tensor([1., 0.]) tensor([0.4860, 0.5140], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 157: cat - cat || Loss: 0.23609308898448944\n",
      "tensor([0., 1.]) tensor([0.4859, 0.5141], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 158: cat - cat || Loss: 0.23594847321510315\n",
      "tensor([0., 1.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 159: dog - cat || Loss: 0.2646946907043457\n",
      "tensor([1., 0.]) tensor([0.4855, 0.5145], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 160: dog - cat || Loss: 0.2648029923439026\n",
      "tensor([1., 0.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 161: cat - cat || Loss: 0.2356300950050354\n",
      "tensor([0., 1.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 162: cat - cat || Loss: 0.2355431616306305\n",
      "tensor([0., 1.]) tensor([0.4853, 0.5147], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 163: cat - cat || Loss: 0.23537138104438782\n",
      "tensor([0., 1.]) tensor([0.4852, 0.5148], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 164: cat - cat || Loss: 0.23512350022792816\n",
      "tensor([0., 1.]) tensor([0.4849, 0.5151], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 165: dog - cat || Loss: 0.265669047832489\n",
      "tensor([1., 0.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 166: dog - cat || Loss: 0.265866219997406\n",
      "tensor([1., 0.]) tensor([0.4844, 0.5156], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 167: dog - cat || Loss: 0.2659382224082947\n",
      "tensor([1., 0.]) tensor([0.4843, 0.5157], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 168: cat - cat || Loss: 0.23459245264530182\n",
      "tensor([0., 1.]) tensor([0.4843, 0.5157], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 169: cat - cat || Loss: 0.23453377187252045\n",
      "tensor([0., 1.]) tensor([0.4843, 0.5157], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 170: cat - cat || Loss: 0.23438788950443268\n",
      "tensor([0., 1.]) tensor([0.4841, 0.5159], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 171: cat - cat || Loss: 0.23416359722614288\n",
      "tensor([0., 1.]) tensor([0.4839, 0.5161], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 172: dog - cat || Loss: 0.26666897535324097\n",
      "tensor([1., 0.]) tensor([0.4836, 0.5164], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 173: dog - cat || Loss: 0.2668461799621582\n",
      "tensor([1., 0.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 174: dog - cat || Loss: 0.26689979434013367\n",
      "tensor([1., 0.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 175: cat - cat || Loss: 0.2337067425251007\n",
      "tensor([0., 1.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 176: cat - cat || Loss: 0.2336626499891281\n",
      "tensor([0., 1.]) tensor([0.4834, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 177: dog - cat || Loss: 0.26703110337257385\n",
      "tensor([1., 0.]) tensor([0.4832, 0.5168], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 178: cat - cat || Loss: 0.23350998759269714\n",
      "tensor([0., 1.]) tensor([0.4832, 0.5168], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 179: cat - cat || Loss: 0.23339924216270447\n",
      "tensor([0., 1.]) tensor([0.4831, 0.5169], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 180: cat - cat || Loss: 0.23320691287517548\n",
      "tensor([0., 1.]) tensor([0.4829, 0.5171], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 181: dog - cat || Loss: 0.26766136288642883\n",
      "tensor([1., 0.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 182: cat - cat || Loss: 0.23280155658721924\n",
      "tensor([0., 1.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 183: cat - cat || Loss: 0.2325834035873413\n",
      "tensor([0., 1.]) tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 184: dog - cat || Loss: 0.26835528016090393\n",
      "tensor([1., 0.]) tensor([0.4820, 0.5180], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 185: cat - cat || Loss: 0.2321343719959259\n",
      "tensor([0., 1.]) tensor([0.4818, 0.5182], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 186: dog - cat || Loss: 0.2687823176383972\n",
      "tensor([1., 0.]) tensor([0.4816, 0.5184], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 187: dog - cat || Loss: 0.2689046561717987\n",
      "tensor([1., 0.]) tensor([0.4814, 0.5186], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 188: cat - cat || Loss: 0.2317812293767929\n",
      "tensor([0., 1.]) tensor([0.4814, 0.5186], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 189: dog - cat || Loss: 0.2690102159976959\n",
      "tensor([1., 0.]) tensor([0.4813, 0.5187], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 190: cat - cat || Loss: 0.231700137257576\n",
      "tensor([0., 1.]) tensor([0.4814, 0.5186], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 191: dog - cat || Loss: 0.26908108592033386\n",
      "tensor([1., 0.]) tensor([0.4813, 0.5187], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 192: cat - cat || Loss: 0.2316480427980423\n",
      "tensor([0., 1.]) tensor([0.4813, 0.5187], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 193: cat - cat || Loss: 0.23158082365989685\n",
      "tensor([0., 1.]) tensor([0.4812, 0.5188], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 194: dog - cat || Loss: 0.2692883610725403\n",
      "tensor([1., 0.]) tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 195: dog - cat || Loss: 0.2693292498588562\n",
      "tensor([1., 0.]) tensor([0.4810, 0.5190], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 196: cat - cat || Loss: 0.23145519196987152\n",
      "tensor([0., 1.]) tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 197: dog - cat || Loss: 0.26929548382759094\n",
      "tensor([1., 0.]) tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 198: cat - cat || Loss: 0.23149074614048004\n",
      "tensor([0., 1.]) tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 199: cat - cat || Loss: 0.23146097362041473\n",
      "tensor([0., 1.]) tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 200: cat - cat || Loss: 0.23134224116802216\n",
      "tensor([0., 1.]) tensor([0.4810, 0.5190], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 201: cat - cat || Loss: 0.23114357888698578\n",
      "tensor([0., 1.]) tensor([0.4808, 0.5192], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 202: dog - cat || Loss: 0.2698878347873688\n",
      "tensor([1., 0.]) tensor([0.4805, 0.5195], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 203: cat - cat || Loss: 0.23072907328605652\n",
      "tensor([0., 1.]) tensor([0.4803, 0.5197], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 204: dog - cat || Loss: 0.27028322219848633\n",
      "tensor([1., 0.]) tensor([0.4801, 0.5199], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 205: dog - cat || Loss: 0.2703915536403656\n",
      "tensor([1., 0.]) tensor([0.4800, 0.5200], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 206: cat - cat || Loss: 0.23041684925556183\n",
      "tensor([0., 1.]) tensor([0.4800, 0.5200], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 207: dog - cat || Loss: 0.2704719603061676\n",
      "tensor([1., 0.]) tensor([0.4799, 0.5201], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 208: cat - cat || Loss: 0.2303578406572342\n",
      "tensor([0., 1.]) tensor([0.4800, 0.5200], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 209: dog - cat || Loss: 0.27052125334739685\n",
      "tensor([1., 0.]) tensor([0.4799, 0.5201], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 210: dog - cat || Loss: 0.2704819142818451\n",
      "tensor([1., 0.]) tensor([0.4799, 0.5201], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 211: cat - cat || Loss: 0.2304563820362091\n",
      "tensor([0., 1.]) tensor([0.4801, 0.5199], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 212: dog - cat || Loss: 0.27030956745147705\n",
      "tensor([1., 0.]) tensor([0.4801, 0.5199], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 213: dog - cat || Loss: 0.2701757550239563\n",
      "tensor([1., 0.]) tensor([0.4802, 0.5198], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 214: cat - cat || Loss: 0.23081743717193604\n",
      "tensor([0., 1.]) tensor([0.4804, 0.5196], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 215: dog - cat || Loss: 0.2698424458503723\n",
      "tensor([1., 0.]) tensor([0.4805, 0.5195], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 216: cat - cat || Loss: 0.23110240697860718\n",
      "tensor([0., 1.]) tensor([0.4807, 0.5193], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 217: cat - cat || Loss: 0.2311791032552719\n",
      "tensor([0., 1.]) tensor([0.4808, 0.5192], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 218: dog - cat || Loss: 0.26958197355270386\n",
      "tensor([1., 0.]) tensor([0.4808, 0.5192], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 219: dog - cat || Loss: 0.26949700713157654\n",
      "tensor([1., 0.]) tensor([0.4809, 0.5191], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 220: cat - cat || Loss: 0.23140491545200348\n",
      "tensor([0., 1.]) tensor([0.4810, 0.5190], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 221: dog - cat || Loss: 0.2692476809024811\n",
      "tensor([1., 0.]) tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 222: cat - cat || Loss: 0.23162014782428741\n",
      "tensor([0., 1.]) tensor([0.4813, 0.5187], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 223: dog - cat || Loss: 0.2690311372280121\n",
      "tensor([1., 0.]) tensor([0.4813, 0.5187], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 224: cat - cat || Loss: 0.23180818557739258\n",
      "tensor([0., 1.]) tensor([0.4815, 0.5185], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 225: cat - cat || Loss: 0.23184330761432648\n",
      "tensor([0., 1.]) tensor([0.4815, 0.5185], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 226: dog - cat || Loss: 0.2689061760902405\n",
      "tensor([1., 0.]) tensor([0.4814, 0.5186], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 227: dog - cat || Loss: 0.2688579857349396\n",
      "tensor([1., 0.]) tensor([0.4815, 0.5185], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 228: dog - cat || Loss: 0.2687080204486847\n",
      "tensor([1., 0.]) tensor([0.4816, 0.5184], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 229: dog - cat || Loss: 0.2684662640094757\n",
      "tensor([1., 0.]) tensor([0.4819, 0.5181], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 230: cat - cat || Loss: 0.232493057847023\n",
      "tensor([0., 1.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 231: cat - cat || Loss: 0.23267221450805664\n",
      "tensor([0., 1.]) tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 232: dog - cat || Loss: 0.26787620782852173\n",
      "tensor([1., 0.]) tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 233: cat - cat || Loss: 0.2329021841287613\n",
      "tensor([0., 1.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 234: cat - cat || Loss: 0.23295477032661438\n",
      "tensor([0., 1.]) tensor([0.4827, 0.5173], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 235: cat - cat || Loss: 0.23290961980819702\n",
      "tensor([0., 1.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 236: cat - cat || Loss: 0.23277661204338074\n",
      "tensor([0., 1.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 237: dog - cat || Loss: 0.2680656611919403\n",
      "tensor([1., 0.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 238: dog - cat || Loss: 0.26816412806510925\n",
      "tensor([1., 0.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 239: dog - cat || Loss: 0.26814645528793335\n",
      "tensor([1., 0.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 240: dog - cat || Loss: 0.26802390813827515\n",
      "tensor([1., 0.]) tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 241: dog - cat || Loss: 0.26780736446380615\n",
      "tensor([1., 0.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 242: cat - cat || Loss: 0.23308612406253815\n",
      "tensor([0., 1.]) tensor([0.4828, 0.5172], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 243: cat - cat || Loss: 0.2332465648651123\n",
      "tensor([0., 1.]) tensor([0.4830, 0.5170], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 244: cat - cat || Loss: 0.23329824209213257\n",
      "tensor([0., 1.]) tensor([0.4830, 0.5170], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 245: cat - cat || Loss: 0.23325219750404358\n",
      "tensor([0., 1.]) tensor([0.4830, 0.5170], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 246: cat - cat || Loss: 0.23311826586723328\n",
      "tensor([0., 1.]) tensor([0.4828, 0.5172], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 247: cat - cat || Loss: 0.23290523886680603\n",
      "tensor([0., 1.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 248: dog - cat || Loss: 0.2680048644542694\n",
      "tensor([1., 0.]) tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 249: dog - cat || Loss: 0.2681727409362793\n",
      "tensor([1., 0.]) tensor([0.4821, 0.5179], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 250: dog - cat || Loss: 0.268217533826828\n",
      "tensor([1., 0.]) tensor([0.4821, 0.5179], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 251: cat - cat || Loss: 0.23248471319675446\n",
      "tensor([0., 1.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 252: cat - cat || Loss: 0.23244787752628326\n",
      "tensor([0., 1.]) tensor([0.4821, 0.5179], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 253: dog - cat || Loss: 0.2683255672454834\n",
      "tensor([1., 0.]) tensor([0.4820, 0.5180], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 254: cat - cat || Loss: 0.2323087751865387\n",
      "tensor([0., 1.]) tensor([0.4820, 0.5180], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 255: dog - cat || Loss: 0.2684527039527893\n",
      "tensor([1., 0.]) tensor([0.4819, 0.5181], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 256: dog - cat || Loss: 0.2684473693370819\n",
      "tensor([1., 0.]) tensor([0.4819, 0.5181], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 257: dog - cat || Loss: 0.2683360278606415\n",
      "tensor([1., 0.]) tensor([0.4820, 0.5180], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 258: dog - cat || Loss: 0.2681291997432709\n",
      "tensor([1., 0.]) tensor([0.4822, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 259: cat - cat || Loss: 0.23277774453163147\n",
      "tensor([0., 1.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 260: cat - cat || Loss: 0.23293060064315796\n",
      "tensor([0., 1.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 261: cat - cat || Loss: 0.23297570645809174\n",
      "tensor([0., 1.]) tensor([0.4827, 0.5173], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 262: cat - cat || Loss: 0.23292389512062073\n",
      "tensor([0., 1.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 263: dog - cat || Loss: 0.2678292691707611\n",
      "tensor([1., 0.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 264: cat - cat || Loss: 0.2327587604522705\n",
      "tensor([0., 1.]) tensor([0.4825, 0.5175], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 265: dog - cat || Loss: 0.2679814100265503\n",
      "tensor([1., 0.]) tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 266: dog - cat || Loss: 0.2679869532585144\n",
      "tensor([1., 0.]) tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 267: dog - cat || Loss: 0.26788559556007385\n",
      "tensor([1., 0.]) tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 268: cat - cat || Loss: 0.23291656374931335\n",
      "tensor([0., 1.]) tensor([0.4826, 0.5174], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 269: dog - cat || Loss: 0.26760929822921753\n",
      "tensor([1., 0.]) tensor([0.4827, 0.5173], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 270: cat - cat || Loss: 0.23315513134002686\n",
      "tensor([0., 1.]) tensor([0.4829, 0.5171], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 271: dog - cat || Loss: 0.26737216114997864\n",
      "tensor([1., 0.]) tensor([0.4829, 0.5171], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 272: cat - cat || Loss: 0.23336097598075867\n",
      "tensor([0., 1.]) tensor([0.4831, 0.5169], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 273: dog - cat || Loss: 0.2671669125556946\n",
      "tensor([1., 0.]) tensor([0.4831, 0.5169], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 274: dog - cat || Loss: 0.2670203149318695\n",
      "tensor([1., 0.]) tensor([0.4833, 0.5167], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 275: dog - cat || Loss: 0.2667825222015381\n",
      "tensor([1., 0.]) tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 276: cat - cat || Loss: 0.23406222462654114\n",
      "tensor([0., 1.]) tensor([0.4838, 0.5162], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 277: dog - cat || Loss: 0.2662740647792816\n",
      "tensor([1., 0.]) tensor([0.4840, 0.5160], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 278: dog - cat || Loss: 0.26599881052970886\n",
      "tensor([1., 0.]) tensor([0.4842, 0.5158], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 279: dog - cat || Loss: 0.26564568281173706\n",
      "tensor([1., 0.]) tensor([0.4846, 0.5154], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 280: cat - cat || Loss: 0.23522719740867615\n",
      "tensor([0., 1.]) tensor([0.4850, 0.5150], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 281: cat - cat || Loss: 0.23549233376979828\n",
      "tensor([0., 1.]) tensor([0.4853, 0.5147], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 282: dog - cat || Loss: 0.26478734612464905\n",
      "tensor([1., 0.]) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 283: dog - cat || Loss: 0.26454371213912964\n",
      "tensor([1., 0.]) tensor([0.4857, 0.5143], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 284: dog - cat || Loss: 0.2642197012901306\n",
      "tensor([1., 0.]) tensor([0.4860, 0.5140], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 285: dog - cat || Loss: 0.2638235092163086\n",
      "tensor([1., 0.]) tensor([0.4864, 0.5136], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 286: dog - cat || Loss: 0.26336240768432617\n",
      "tensor([1., 0.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 287: dog - cat || Loss: 0.26284345984458923\n",
      "tensor([1., 0.]) tensor([0.4873, 0.5127], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 288: cat - cat || Loss: 0.23802144825458527\n",
      "tensor([0., 1.]) tensor([0.4879, 0.5121], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 289: dog - cat || Loss: 0.26185840368270874\n",
      "tensor([1., 0.]) tensor([0.4883, 0.5117], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 290: dog - cat || Loss: 0.26138219237327576\n",
      "tensor([1., 0.]) tensor([0.4887, 0.5113], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 291: cat - cat || Loss: 0.23938024044036865\n",
      "tensor([0., 1.]) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 292: dog - cat || Loss: 0.2604711055755615\n",
      "tensor([1., 0.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 293: cat - cat || Loss: 0.24017037451267242\n",
      "tensor([0., 1.]) tensor([0.4901, 0.5099], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 294: dog - cat || Loss: 0.2597263753414154\n",
      "tensor([1., 0.]) tensor([0.4904, 0.5096], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 295: cat - cat || Loss: 0.24081864953041077\n",
      "tensor([0., 1.]) tensor([0.4907, 0.5093], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 296: dog - cat || Loss: 0.2591165006160736\n",
      "tensor([1., 0.]) tensor([0.4910, 0.5090], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 297: cat - cat || Loss: 0.24135127663612366\n",
      "tensor([0., 1.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 298: cat - cat || Loss: 0.24152977764606476\n",
      "tensor([0., 1.]) tensor([0.4915, 0.5085], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 299: cat - cat || Loss: 0.2415946125984192\n",
      "tensor([0., 1.]) tensor([0.4915, 0.5085], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 300: cat - cat || Loss: 0.24155709147453308\n",
      "tensor([0., 1.]) tensor([0.4915, 0.5085], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 301: dog - cat || Loss: 0.25872206687927246\n",
      "tensor([1., 0.]) tensor([0.4914, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 302: dog - cat || Loss: 0.2587401270866394\n",
      "tensor([1., 0.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 303: cat - cat || Loss: 0.24149352312088013\n",
      "tensor([0., 1.]) tensor([0.4914, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 304: cat - cat || Loss: 0.24147284030914307\n",
      "tensor([0., 1.]) tensor([0.4914, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 305: cat - cat || Loss: 0.24135836958885193\n",
      "tensor([0., 1.]) tensor([0.4913, 0.5087], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 306: dog - cat || Loss: 0.25899943709373474\n",
      "tensor([1., 0.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 307: dog - cat || Loss: 0.2590819299221039\n",
      "tensor([1., 0.]) tensor([0.4910, 0.5090], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 308: dog - cat || Loss: 0.25905343890190125\n",
      "tensor([1., 0.]) tensor([0.4910, 0.5090], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 309: cat - cat || Loss: 0.2412315160036087\n",
      "tensor([0., 1.]) tensor([0.4912, 0.5088], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 310: cat - cat || Loss: 0.24124734103679657\n",
      "tensor([0., 1.]) tensor([0.4912, 0.5088], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 311: cat - cat || Loss: 0.2411658763885498\n",
      "tensor([0., 1.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 312: dog - cat || Loss: 0.2591680884361267\n",
      "tensor([1., 0.]) tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 313: cat - cat || Loss: 0.24094410240650177\n",
      "tensor([0., 1.]) tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 314: cat - cat || Loss: 0.24080094695091248\n",
      "tensor([0., 1.]) tensor([0.4907, 0.5093], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 315: cat - cat || Loss: 0.2405766099691391\n",
      "tensor([0., 1.]) tensor([0.4905, 0.5095], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 316: dog - cat || Loss: 0.2599133253097534\n",
      "tensor([1., 0.]) tensor([0.4902, 0.5098], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 317: dog - cat || Loss: 0.26008835434913635\n",
      "tensor([1., 0.]) tensor([0.4900, 0.5100], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 318: dog - cat || Loss: 0.2601426839828491\n",
      "tensor([1., 0.]) tensor([0.4900, 0.5100], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 319: cat - cat || Loss: 0.24011120200157166\n",
      "tensor([0., 1.]) tensor([0.4900, 0.5100], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 320: cat - cat || Loss: 0.24006284773349762\n",
      "tensor([0., 1.]) tensor([0.4900, 0.5100], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 321: cat - cat || Loss: 0.23992420732975006\n",
      "tensor([0., 1.]) tensor([0.4898, 0.5102], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 322: cat - cat || Loss: 0.23970428109169006\n",
      "tensor([0., 1.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 323: cat - cat || Loss: 0.23941130936145782\n",
      "tensor([0., 1.]) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 324: cat - cat || Loss: 0.2390529215335846\n",
      "tensor([0., 1.]) tensor([0.4889, 0.5111], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 325: cat - cat || Loss: 0.23863585293293\n",
      "tensor([0., 1.]) tensor([0.4885, 0.5115], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 326: cat - cat || Loss: 0.23816616833209991\n",
      "tensor([0., 1.]) tensor([0.4880, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 327: dog - cat || Loss: 0.26266342401504517\n",
      "tensor([1., 0.]) tensor([0.4875, 0.5125], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 328: dog - cat || Loss: 0.263047993183136\n",
      "tensor([1., 0.]) tensor([0.4871, 0.5129], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 329: dog - cat || Loss: 0.2632898688316345\n",
      "tensor([1., 0.]) tensor([0.4869, 0.5131], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 330: dog - cat || Loss: 0.2634032964706421\n",
      "tensor([1., 0.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 331: cat - cat || Loss: 0.2369491159915924\n",
      "tensor([0., 1.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 332: dog - cat || Loss: 0.26349762082099915\n",
      "tensor([1., 0.]) tensor([0.4867, 0.5133], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 333: dog - cat || Loss: 0.2634803056716919\n",
      "tensor([1., 0.]) tensor([0.4867, 0.5133], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 334: cat - cat || Loss: 0.23698757588863373\n",
      "tensor([0., 1.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 335: dog - cat || Loss: 0.263351172208786\n",
      "tensor([1., 0.]) tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 336: dog - cat || Loss: 0.2632386088371277\n",
      "tensor([1., 0.]) tensor([0.4869, 0.5131], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 337: dog - cat || Loss: 0.263033002614975\n",
      "tensor([1., 0.]) tensor([0.4871, 0.5129], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 338: cat - cat || Loss: 0.23757314682006836\n",
      "tensor([0., 1.]) tensor([0.4874, 0.5126], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 339: cat - cat || Loss: 0.23772646486759186\n",
      "tensor([0., 1.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 340: cat - cat || Loss: 0.2377701848745346\n",
      "tensor([0., 1.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 341: dog - cat || Loss: 0.2625943124294281\n",
      "tensor([1., 0.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 342: dog - cat || Loss: 0.26254206895828247\n",
      "tensor([1., 0.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 343: cat - cat || Loss: 0.23790864646434784\n",
      "tensor([0., 1.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 344: cat - cat || Loss: 0.23794367909431458\n",
      "tensor([0., 1.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 345: cat - cat || Loss: 0.2378809154033661\n",
      "tensor([0., 1.]) tensor([0.4877, 0.5123], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 346: dog - cat || Loss: 0.26257872581481934\n",
      "tensor([1., 0.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 347: dog - cat || Loss: 0.26261723041534424\n",
      "tensor([1., 0.]) tensor([0.4875, 0.5125], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 348: dog - cat || Loss: 0.26254767179489136\n",
      "tensor([1., 0.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 349: cat - cat || Loss: 0.23791822791099548\n",
      "tensor([0., 1.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 350: cat - cat || Loss: 0.2379665970802307\n",
      "tensor([0., 1.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 351: dog - cat || Loss: 0.26238352060317993\n",
      "tensor([1., 0.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 352: cat - cat || Loss: 0.23796920478343964\n",
      "tensor([0., 1.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 353: cat - cat || Loss: 0.23792287707328796\n",
      "tensor([0., 1.]) tensor([0.4878, 0.5122], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 354: dog - cat || Loss: 0.262518972158432\n",
      "tensor([1., 0.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 355: dog - cat || Loss: 0.26254361867904663\n",
      "tensor([1., 0.]) tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 356: cat - cat || Loss: 0.23784160614013672\n",
      "tensor([0., 1.]) tensor([0.4877, 0.5123], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 357: dog - cat || Loss: 0.2624867558479309\n",
      "tensor([1., 0.]) tensor([0.4877, 0.5123], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 358: dog - cat || Loss: 0.2624053359031677\n",
      "tensor([1., 0.]) tensor([0.4877, 0.5123], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 359: dog - cat || Loss: 0.26222798228263855\n",
      "tensor([1., 0.]) tensor([0.4879, 0.5121], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 360: cat - cat || Loss: 0.23831519484519958\n",
      "tensor([0., 1.]) tensor([0.4882, 0.5118], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 361: cat - cat || Loss: 0.23844683170318604\n",
      "tensor([0., 1.]) tensor([0.4883, 0.5117], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 362: dog - cat || Loss: 0.26180151104927063\n",
      "tensor([1., 0.]) tensor([0.4883, 0.5117], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 363: dog - cat || Loss: 0.261675089597702\n",
      "tensor([1., 0.]) tensor([0.4885, 0.5115], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 364: cat - cat || Loss: 0.2387991100549698\n",
      "tensor([0., 1.]) tensor([0.4887, 0.5113], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 365: cat - cat || Loss: 0.2388913929462433\n",
      "tensor([0., 1.]) tensor([0.4888, 0.5112], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 366: dog - cat || Loss: 0.26137328147888184\n",
      "tensor([1., 0.]) tensor([0.4888, 0.5112], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 367: dog - cat || Loss: 0.26128053665161133\n",
      "tensor([1., 0.]) tensor([0.4888, 0.5112], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 368: dog - cat || Loss: 0.2610936760902405\n",
      "tensor([1., 0.]) tensor([0.4890, 0.5110], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 2 - 369: dog - cat || Loss: 0.2608218491077423\n",
      "tensor([1., 0.]) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)\n",
      "=====epoch:3=====\n",
      "Epoch 3 - 0: dog - cat || Loss: 0.26047393679618835\n",
      "tensor([1., 0.]) tensor([0.4896, 0.5104], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 3 - 1: dog - cat || Loss: 0.26005783677101135\n",
      "tensor([1., 0.]) tensor([0.4900, 0.5100], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 3 - 2: cat - cat || Loss: 0.2405996471643448\n",
      "tensor([0., 1.]) tensor([0.4905, 0.5095], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 3 - 3: cat - cat || Loss: 0.2409176528453827\n",
      "tensor([0., 1.]) tensor([0.4908, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 3 - 4: cat - cat || Loss: 0.2411084622144699\n",
      "tensor([0., 1.]) tensor([0.4910, 0.5090], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 3 - 5: cat - cat || Loss: 0.24118462204933167\n",
      "tensor([0., 1.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 3 - 6: cat - cat || Loss: 0.24115745723247528\n",
      "tensor([0., 1.]) tensor([0.4911, 0.5089], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 3 - 7: dog - cat || Loss: 0.259126216173172\n",
      "tensor([1., 0.]) tensor([0.4910, 0.5090], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch 3 - 8: cat - cat || Loss: 0.24102848768234253\n",
      "tensor([0., 1.]) tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m   number_of_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_label(y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_label(y_hat)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m || Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ClassifierVGG16()\n",
    "print(model(x))\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "loss_calculator = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "  number_of_example = len(dataset)\n",
    "  number_of_correct = 0\n",
    "  print(f\"=====epoch:{epoch}=====\")\n",
    "  for id in range(number_of_example):\n",
    "    x, y = dataset[id]\n",
    "    y_hat = model(x)\n",
    "\n",
    "    loss = ((y - y_hat) ** 2).mean()\n",
    "    # loss = loss_calculator(y, y_hat)\n",
    "\n",
    "    if get_label(y_hat) == get_label(y):\n",
    "      number_of_correct += 1\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch} - {id}: {get_label(y)} - {get_label(y_hat)} || Loss: {loss.item()}\")\n",
    "    print(y, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeh8+7BqFjyDtE333ycs6T",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
